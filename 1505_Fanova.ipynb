{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: likelihood.noise_covar.raw_noise           value = tensor([-3.1533])\n",
      "Parameter name: base_kernel.raw_lengthscale                value = tensor([[0.6244]])\n",
      "Parameter name: covar_module.raw_outputscale               value = tensor([[-0.7119, -4.8627, -2.2748, -8.3249, -9.2394, -9.0218, -7.6204, -2.4694]])\n",
      "Constraint name: likelihood.noise_covar.raw_noise_constraint             constraint = GreaterThan(1.000E-04)\n",
      "Constraint name: base_kernel.raw_lengthscale_constraint                  constraint = Positive()\n",
      "Constraint name: covar_module.raw_outputscale_constraint                 constraint = Positive()\n",
      "Predicted Means:\n",
      "[1.0501919  1.5123014  1.0332913  0.7673197  1.0754166  2.9719605\n",
      " 1.9691772  1.5605273  2.5621562  1.9208918  3.3095746  1.3127961\n",
      " 1.3040628  3.264596   1.5741014  0.41906548 2.1697662  1.6299438\n",
      " 1.458086   1.2952032  1.6194601  0.8747935  1.8426151  2.360249\n",
      " 2.461608   3.5174327  1.1199017  3.1491973  1.9761467  1.5538125\n",
      " 2.8810065  0.63751507 2.6201196  1.1580648  2.4031527  3.6151552\n",
      " 1.9613523  1.8435879  1.0463223  2.4863138  0.6734228  2.632146  ]\n",
      "Predicted Standard Deviations:\n",
      "[0.2761536  0.33428854 0.32307073 0.32370704 0.38182527 0.85181814\n",
      " 0.23297884 0.25912368 0.27859864 0.5148634  0.39705336 0.3915493\n",
      " 0.3211308  0.45754844 0.26322648 0.98637265 0.2751808  0.24822423\n",
      " 0.36569756 0.41294292 0.33412215 0.25164148 0.2550788  0.4078757\n",
      " 0.29623336 0.423637   0.30946234 0.6868291  0.26679447 0.35436583\n",
      " 0.5933378  0.41588324 0.37460548 0.71194226 0.3880835  0.41899085\n",
      " 0.25418502 0.72852093 0.31931263 0.24968249 0.31588253 0.27740502]\n",
      "x_train torch.Size([168, 8])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score,train_test_split\n",
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "\n",
    "import torch\n",
    "from linear_operator import to_dense\n",
    "from gpytorch.constraints import Positive\n",
    "from gpytorch.kernels import Kernel\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "\n",
    "# diabetes = load_diabetes()\n",
    "# X, y = diabetes.data, diabetes.target\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "X = X[:210, :]\n",
    "y = y[:210]\n",
    "# X = X[:, :3]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_x, test_x, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 20% data as test\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform it\n",
    "train_x = scaler.fit_transform(train_x)\n",
    "\n",
    "# Transform the test data using the same scaler\n",
    "test_x = scaler.transform(test_x)\n",
    "\n",
    "# Convert to torch tensors\n",
    "train_x = torch.tensor(train_x,dtype=torch.float32)\n",
    "test_x = torch.tensor(test_x, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "k(x_1, x_2) = \\exp\\left(-\\frac{1}{2 \\l^2} \\sum_{k=1}^d \\left(\\left| x_{1k} - x_{2k} \\right|\\right)^2 \\right)\n",
    "'''\n",
    "    \n",
    "class CustomRBFKernel(gpytorch.kernels.Kernel):\n",
    "\n",
    "    has_lengthscale = True\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        # Compute squared distance\n",
    "        # squared_dist = self.covar_dist(x1, x2, square_dist=True, diag=diag, **params)\n",
    "        diff = x1.unsqueeze(1) - x2.unsqueeze(0)\n",
    "        diff = torch.abs(diff)\n",
    "        squared_dist = (diff ** 2).sum(-1)\n",
    "\n",
    "        # Divide by 2 * lengthscale^2\n",
    "        scaled_squared_dist = squared_dist.div(2 * self.lengthscale.pow(2)) #.div\n",
    "\n",
    "        # Compute exponential\n",
    "        covar_matrix = scaled_squared_dist.mul_(-1).exp_()\n",
    "\n",
    "        return covar_matrix\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "class DPkernel(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, base_kernel, num_dims, q_additivity, train_x, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.base_kernel = base_kernel\n",
    "        self.num_dims = num_dims\n",
    "        self.q_additivity = q_additivity\n",
    "        self.register_parameter(\n",
    "            name=\"raw_outputscale\", \n",
    "            parameter=torch.nn.Parameter(torch.zeros(1, self.q_additivity))\n",
    "        )\n",
    "        self.outputscale_constraint = gpytorch.constraints.Positive()\n",
    "        self.register_constraint(\"raw_outputscale\", self.outputscale_constraint)\n",
    "        \n",
    "        # Extract mean and standard deviation\n",
    "        self.mu = torch.mean(train_x, dim=0)  # Mean of train_x\n",
    "        self.delta = torch.std(train_x, dim=0)  # Standard deviation of train_x\n",
    "\n",
    "    @property\n",
    "    def outputscale(self):\n",
    "        return self.outputscale_constraint.transform(self.raw_outputscale).squeeze()\n",
    "\n",
    "    @outputscale.setter\n",
    "    def outputscale(self, value):\n",
    "        if not torch.is_tensor(value):\n",
    "            value = torch.tensor(value, device=self.raw_outputscale.device)\n",
    "        self.initialize(raw_outputscale=self.outputscale_constraint.inverse_transform(value))\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        x1_size = x1.size(0)\n",
    "        x2_size = x2.size(0)\n",
    "        \n",
    "        result = torch.zeros(x1_size, x2_size, device=x1.device)\n",
    "        sum_order_b = torch.zeros(x1_size, x2_size, device=x1.device)\n",
    "        kernels = []\n",
    "\n",
    "        # First order calculations with additional term\n",
    "        for d in range(self.num_dims):\n",
    "            x1_d = x1[:, d:d+1]\n",
    "            x2_d = x2[:, d:d+1]\n",
    "            k_d = self.base_kernel(x1_d, x2_d).evaluate()\n",
    "            kernels.append(k_d)\n",
    "            sum_order_b += k_d\n",
    "\n",
    "            # Additional term based on the formula form the paper Additice Gaussian Process\n",
    "            diff = x1_d.unsqueeze(1) - x2_d.unsqueeze(0)\n",
    "            squared_dist = (diff ** 2).sum(-1)\n",
    "            scaled_squared_dist = squared_dist.div(2 * self.base_kernel.lengthscale.pow(2))\n",
    "\n",
    "            mu_d = self.mu[d]\n",
    "            delta_d = self.delta[d]\n",
    "            mu_term = ((x1_d - mu_d) ** 2).sum(-1, keepdim=True) + ((x2_d - mu_d) ** 2).sum(-1)\n",
    "            mu_term = mu_term.div(2 * (self.base_kernel.lengthscale.pow(2) + delta_d.pow(2)))\n",
    "\n",
    "            exp_term_1 = (-scaled_squared_dist).exp()\n",
    "            exp_term_2 = (-mu_term).exp()\n",
    "\n",
    "            additional_term = self.base_kernel.lengthscale.sqrt() * (self.base_kernel.lengthscale.pow(2) + 2 * delta_d.pow(2)).sqrt()\n",
    "            additional_term = additional_term.div(self.base_kernel.lengthscale.pow(2) + delta_d.pow(2))\n",
    "            additional_term = additional_term * exp_term_2\n",
    "\n",
    "            sum_order_b += exp_term_1 - additional_term\n",
    "        \n",
    "        outputscale = self.outputscale.unsqueeze(0) if len(self.outputscale.shape) == 0 else self.outputscale\n",
    "        result += sum_order_b * self.outputscale[0]\n",
    "\n",
    "        # Compute higher order interactions\n",
    "        for i in range(1, self.q_additivity):\n",
    "            temp_sum = torch.zeros(x1_size, x2_size, device=x1.device)\n",
    "            new_kernels = []\n",
    "            for j in range(self.num_dims):\n",
    "                for k in range(j + 1, self.num_dims):\n",
    "                    new_kernel = kernels[j] * kernels[k]\n",
    "                    new_kernels.append(new_kernel)\n",
    "                    temp_sum += new_kernel\n",
    "\n",
    "            kernels = new_kernels\n",
    "            result += temp_sum * self.outputscale[i]\n",
    "\n",
    "        return result\n",
    "    \n",
    "class DPkernel(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, base_kernel, num_dims, q_additivity, train_sample, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.base_kernel = base_kernel\n",
    "        self.num_dims = num_dims\n",
    "        self.q_additivity = q_additivity\n",
    "        self.train_sample = train_sample\n",
    "        self.register_parameter(\n",
    "            name=\"raw_outputscale\", \n",
    "            parameter=torch.nn.Parameter(torch.zeros(1, self.q_additivity))\n",
    "        )\n",
    "        self.outputscale_constraint = gpytorch.constraints.Positive()\n",
    "        self.register_constraint(\"raw_outputscale\", self.outputscale_constraint)\n",
    "        \n",
    "        # Extract mean and standard deviation\n",
    "        self.mu = torch.mean(self.train_sample, dim=0)  # Mean of train_x\n",
    "        self.delta = torch.std(self.train_sample, dim=0)  # Standard deviation of train_x\n",
    "\n",
    "    @property\n",
    "    def outputscale(self):\n",
    "        return self.outputscale_constraint.transform(self.raw_outputscale).squeeze()\n",
    "\n",
    "    @outputscale.setter\n",
    "    def outputscale(self, value):\n",
    "        if not torch.is_tensor(value):\n",
    "            value = torch.tensor(value, device=self.raw_outputscale.device)\n",
    "        self.initialize(raw_outputscale=self.outputscale_constraint.inverse_transform(value))\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        x1_size = x1.size(0)\n",
    "        x2_size = x2.size(0)\n",
    "        \n",
    "        result = torch.zeros(x1_size, x2_size, device=x1.device)\n",
    "        sum_order_b = torch.zeros(x1_size, x2_size, device=x1.device)\n",
    "        kernels = []\n",
    "\n",
    "        # First order calculations with additional term\n",
    "        for d in range(self.num_dims):\n",
    "            x1_d = x1[:, d:d+1]\n",
    "            x2_d = x2[:, d:d+1]\n",
    "            # first term form base kernel\n",
    "            k_d = self.base_kernel(x1_d, x2_d).evaluate()\n",
    "            # kernels.append(k_d)\n",
    "            # sum_order_b += k_d\n",
    "\n",
    "            # Additional term based on the formula in the image for the first order\n",
    "            #first term\n",
    "            \n",
    "\n",
    "            #second term\n",
    "            mu_d = self.mu[d]\n",
    "            delta_d = self.delta[d]\n",
    "            mu_term = ((x1_d - mu_d) ** 2).sum(-1, keepdim=True) + ((x2_d - mu_d) ** 2).sum(-1)\n",
    "            mu_term = mu_term.div(2 * (self.base_kernel.lengthscale.pow(2) + delta_d.pow(2)))\n",
    "            exp_term_2 = (-mu_term).exp()\n",
    "\n",
    "            middle_term = self.base_kernel.lengthscale * torch.sqrt(self.base_kernel.lengthscale.pow(2) + 2 * delta_d.pow(2)).sqrt()\n",
    "            middle_term = middle_term.div(self.base_kernel.lengthscale.pow(2) + delta_d.pow(2))\n",
    "            additional_term = middle_term * exp_term_2\n",
    "\n",
    "            k0 = k_d - additional_term\n",
    "            kernels.append(k0)\n",
    "\n",
    "            sum_order_b += k0\n",
    "        \n",
    "        outputscale = self.outputscale.unsqueeze(0) if len(self.outputscale.shape) == 0 else self.outputscale\n",
    "        result += sum_order_b * self.outputscale[0]\n",
    "\n",
    "        # Compute higher order interactions\n",
    "        for i in range(1, self.q_additivity):\n",
    "            temp_sum = torch.zeros(x1_size, x2_size, device=x1.device)\n",
    "            new_kernels = []\n",
    "            for j in range(self.num_dims):\n",
    "                for k in range(j + 1, self.num_dims):\n",
    "                    new_kernel = kernels[j] * kernels[k]\n",
    "                    new_kernels.append(new_kernel)\n",
    "                    temp_sum += new_kernel\n",
    "\n",
    "            kernels = new_kernels\n",
    "            result += temp_sum * self.outputscale[i]\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "# class DPkernel(gpytorch.kernels.Kernel):\n",
    "#     def __init__(self, base_kernel, num_dims, q_additivity, **kwargs):\n",
    "#         super().__init__(**kwargs)\n",
    "#         self.base_kernel = base_kernel\n",
    "#         self.num_dims = num_dims\n",
    "#         self.q_additivity = q_additivity\n",
    "#         self.register_parameter(\n",
    "#             name=\"raw_outputscale\", \n",
    "#             parameter=torch.nn.Parameter(torch.zeros(1, self.q_additivity))\n",
    "#         )\n",
    "#         self.outputscale_constraint = gpytorch.constraints.Positive()\n",
    "#         self.register_constraint(\"raw_outputscale\", self.outputscale_constraint)\n",
    "\n",
    "#     @property\n",
    "#     def outputscale(self):\n",
    "#         return self.outputscale_constraint.transform(self.raw_outputscale).squeeze()\n",
    "\n",
    "#     @outputscale.setter\n",
    "#     def outputscale(self, value):\n",
    "#         if not torch.is_tensor(value):\n",
    "#             value = torch.tensor(value, device=self.raw_outputscale.device)\n",
    "#         self.initialize(raw_outputscale=self.outputscale_constraint.inverse_transform(value))\n",
    "\n",
    "#     def forward(self, x1, x2, diag=False, **params):\n",
    "#     # Determine sizes based on input matrices\n",
    "#         x1_size = x1.size(0)\n",
    "#         x2_size = x2.size(0)\n",
    "        \n",
    "#         # Initialize matrices based on input sizes\n",
    "#         result = torch.zeros(x1_size, x2_size, device=x1.device) #initialize the result matrix\n",
    "#         sum_order_b = torch.zeros(x1_size, x2_size, device=x1.device) # initialize the matrix for the matrix for a single order\n",
    "#         kernels =[] # list were the z1, z2,... would be stored\n",
    "\n",
    "#         # print(f\"Initial x1 shape: {x1.shape}, x2 shape: {x2.shape}\")\n",
    "        \n",
    "#         #calculations for first order\n",
    "#         #calcualte the kernels for each dimentions\n",
    "#         for d in range(self.num_dims):\n",
    "#             x1_d = x1[:, d:d+1]\n",
    "#             x2_d = x2[:, d:d+1]\n",
    "#             k_d = self.base_kernel(x1_d, x2_d).evaluate() # change thek to k0\n",
    "            \n",
    "#             kernels.append(k_d) #save them in order in the kernels list\n",
    "#             # print(f\"Kernel k_d at dim {d} shape: {k_d.shape}, sum_order_b shape: {sum_order_b.shape}\")\n",
    "\n",
    "#             sum_order_b += k_d # add each one dimension kernels to one matrix for first order\n",
    "    \n",
    "#         # first_kernels = kernels\n",
    "#         outputscale = self.outputscale.unsqueeze(0) if len(self.outputscale.shape) == 0 else self.outputscale\n",
    "#         result += sum_order_b * self.outputscale[0] #add the first order kernel miltiplied by first outputscale\n",
    "\n",
    "#         # Compute higher order interactions\n",
    "#         for i in range(1, self.q_additivity):\n",
    "#             temp_sum = torch.zeros(x1_size, x2_size, device=x1.device)\n",
    "#             new_kernels = []\n",
    "#             for j in range(self.num_dims):\n",
    "#                 for k in range(j + 1, self.num_dims):\n",
    "#                     new_kernel = kernels[j] * kernels[k]\n",
    "#                     new_kernels.append(new_kernel)\n",
    "#                     temp_sum += new_kernel\n",
    "\n",
    "#             kernels = new_kernels  # update kernels list with new order interactions\n",
    "#             result += temp_sum * self.outputscale[i]\n",
    "\n",
    "#         return result\n",
    "\n",
    "# Example usage in a GP model\n",
    "class MyGP(gpytorch.models.ExactGP): # i need to find a diferent model\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(MyGP, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        # self.base_kernel = gpytorch.kernels.RBFKernel()\n",
    "        self.base_kernel = CustomRBFKernel()\n",
    "        self.covar_module = DPkernel(base_kernel=self.base_kernel, num_dims=train_x.size(-1), q_additivity=train_x.size(-1), train_sample = train_x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x,x)  # Make sure to pass x twice WHY\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Create the GP model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "\n",
    "model = MyGP(train_x, y_train.squeeze(-1), likelihood)\n",
    "model.eval()\n",
    "# with torch.no_grad():\n",
    "#     untrained_pred_dist = likelihood(model(test_x))\n",
    "#     predictive_mean = untrained_pred_dist.mean\n",
    "#     lower, upper = untrained_pred_dist.confidence_region()\n",
    "# Set up optimizer and marginal log likelihood\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "# Training loop\n",
    "training_iter = 1000\n",
    "for i in range(training_iter):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    # print(output)\n",
    "    loss = -mll(output, y_train)\n",
    "    loss = loss.mean() \n",
    "    loss.backward()\n",
    "    # print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "    #     i + 1, training_iter, loss.item(),\n",
    "    #     model.covar_module.base_kernel.lengthscale.item(),\n",
    "    #     model.likelihood.noise.item()\n",
    "    # ))\n",
    "    optimizer.step()\n",
    "# print('likelihood noise', likelihood.noise)\n",
    "# print('likelihood noise raw', likelihood.noise_covar.raw_noise)\n",
    "model.eval()\n",
    "# with torch.no_grad():\n",
    "#     trained_pred_dist = likelihood(model(test_x))\n",
    "#     predictive_mean = trained_pred_dist.mean\n",
    "#     lower, upper = trained_pred_dist.confidence_region()\n",
    "# Viewing model parameters after training\n",
    "for param_name, param in model.named_parameters():\n",
    "    print(f'Parameter name: {param_name:42} value = {param.data}')\n",
    "\n",
    "\n",
    "#evaluating there is a problem when the test_y and test_x have float numbers\n",
    "with torch.no_grad():\n",
    "    model.eval()  # Set the model to evaluation mode (mode is for computing predictions through the model posterior.)\n",
    "    likelihood.eval()\n",
    "    output = likelihood(model(test_x))  # Make predictions on new data \n",
    "    \n",
    "\n",
    "\n",
    "for constraint_name, constraint in model.named_constraints():\n",
    "    print(f'Constraint name: {constraint_name:55} constraint = {constraint}')\n",
    "\n",
    "\n",
    "# Extracting means and standard deviations\n",
    "predicted_means = output.mean.numpy() \n",
    "predicted_stddevs = output.stddev.numpy()  # Extract standard deviations\n",
    "\n",
    "print(\"Predicted Means:\")\n",
    "print(predicted_means)\n",
    "\n",
    "print(\"Predicted Standard Deviations:\")\n",
    "print(predicted_stddevs)\n",
    "\n",
    "\n",
    "print(\"x_train\", train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([168, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# calculate teh alpha_hat_eta\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad():\n",
    "    # Evaluate the kernel matrix\n",
    "    t_k_matrix = model.covar_module(train_x).evaluate()\n",
    "    \n",
    "    # Ensure the noise variance is non-zero and sufficiently large to avoid singularity\n",
    "    noise_variance = likelihood.noise_covar.noise if likelihood.noise_covar.noise > 1e-6 else 1e-6\n",
    "    n_matrix = torch.eye(t_k_matrix.size(-1), device=t_k_matrix.device) * noise_variance\n",
    "    \n",
    "    # Add regularization to avoid singular matrix\n",
    "\n",
    "    K_inv = torch.inverse(t_k_matrix + n_matrix)# + torch.eye(t_k_matrix.size(-1), device=t_k_matrix.device))\n",
    "\n",
    "    # Compute alpha_hat_eta using the inverse (dot product)\n",
    "    alpha_hat_eta = torch.matmul(K_inv, y_train).unsqueeze(1)\n",
    "\n",
    "    print(alpha_hat_eta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix K (n*d):\n",
      "torch.Size([168, 8])\n"
     ]
    }
   ],
   "source": [
    "n, d = train_x.shape\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    kernel = model.covar_module\n",
    "\n",
    "    # Initialize the matrix K with zeros\n",
    "    K_per_feature = torch.zeros((n, d))\n",
    "\n",
    "    # Extracting a specific instance's features\n",
    "    instance_features = train_x[3].unsqueeze(0)  # Shape (1, d)\n",
    "\n",
    "    # Loop over each feature dimension\n",
    "    for i in range(d):\n",
    "        # Reshape the specific feature across all samples to match the input shape required by the kernel\n",
    "        feature_column = train_x[:, i].unsqueeze(1)  # Shape (n, 1)\n",
    "        instance_feature = instance_features[:, i].unsqueeze(1)  # Shape (1, 1)\n",
    "\n",
    "        K_per_feature[:, i] = kernel(instance_feature, feature_column).evaluate().squeeze()\n",
    "        # print(K_per_feature[:, i])\n",
    "\n",
    "print(\"Matrix K (n*d):\")\n",
    "print(K_per_feature.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brute Force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the Shapley value: tensor([148.7976, 144.6180, 146.8477, 135.9618, 155.0772, 141.9465, 153.5995,\n",
      "        132.3171])\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain, combinations\n",
    "\n",
    "\n",
    "n_samples, n_features = train_x.size()\n",
    "val = torch.zeros(n_features)\n",
    "\n",
    "# Calculate the extended K\n",
    "temp = range(n_features)\n",
    "feature_combinations = list(chain.from_iterable(combinations(temp, r) for r in range(n_features+1)))\n",
    "feature_combinations.remove(())\n",
    "\n",
    "extended_K = torch.zeros((n_samples, len(feature_combinations)))\n",
    "for i, comb in enumerate(feature_combinations):\n",
    "    product = torch.ones(n_samples)#.unsqueeze(0)\n",
    "\n",
    "    for j in comb:\n",
    "        product = product *K_per_feature[:, j]#.unsqueeze(0)\n",
    "    extended_K[:, i] = product\n",
    "\n",
    "# Create a loop for each feature to compute its Shapley value\n",
    "for j in range(n_features):\n",
    "    # Find the subsets where the j-th feature was used\n",
    "    indices_of_kj_columns = [idx for idx, combination in enumerate(feature_combinations) if j in combination]\n",
    "    indices_of_kj_columns = torch.tensor(indices_of_kj_columns) # Convert the list to a tensor\n",
    "\n",
    "    # Update the extended_K matrix to only include the columns with the indices found\n",
    "    updated_extended_K = torch.zeros(n_samples, len(indices_of_kj_columns))\n",
    "    for i, idx in enumerate(indices_of_kj_columns):\n",
    "        updated_extended_K[:, i] = extended_K[:, idx]\n",
    "\n",
    "    # Create a vector of weights for the corresponding columns\n",
    "    weights = torch.zeros((len(indices_of_kj_columns), 1))\n",
    "\n",
    "    # Set the weights as 1 / length of the combination\n",
    "    for i, idx in enumerate(indices_of_kj_columns):\n",
    "        weights[i] = 1 / len(feature_combinations[idx])\n",
    "        \n",
    "\n",
    "    # Compute omega\n",
    "    omega = torch.matmul(updated_extended_K, weights)\n",
    "    # omega = torch.zeros(n_samples,1)\n",
    "    # for i in range(len(indices_of_kj_columns)):\n",
    "    #     col = (updated_extended_K[:, i] * weights[i]).unsqueeze(1)\n",
    "        \n",
    "    \n",
    "    #     omega += col\n",
    "\n",
    "    # Compute the Shapley value for the j-th feature\n",
    "    val[j] = torch.matmul(omega.T, alpha_hat_eta)\n",
    "\n",
    "print('This is the Shapley value:', val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is val: tensor([148.7844, 144.6147, 146.8480, 135.9597, 155.0752, 141.9478, 153.5898,\n",
      "        132.3176])\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_features = train_x.size()\n",
    "def Omega(X, i, q_additivity=None, feature_type='numerical'):\n",
    "    \n",
    "    \n",
    "    if q_additivity is None:\n",
    "        q_additivity = d\n",
    "    \n",
    "    # Reorder columns so that the i-th column is first\n",
    "    idx = torch.arange(d)\n",
    "    idx[i] = 0\n",
    "    idx[0] = i\n",
    "    X = X[:, idx]\n",
    "\n",
    "    # Initialize dp array\n",
    "    dp = torch.zeros((q_additivity, d, n))\n",
    "\n",
    "    # Initial sum of features across the dataset\n",
    "    sum_current = torch.zeros((n,))\n",
    "    \n",
    "    # Fill the first order dp (base case)\n",
    "    for j in range(d):\n",
    "        dp[0, j, :] = X[:, j]\n",
    "        sum_current += X[:, j]\n",
    "\n",
    "    # Fill the dp table for higher orders\n",
    "    for i in range(1, q_additivity):\n",
    "        temp_sum = torch.zeros((n,))\n",
    "        for j in range(d):\n",
    "            # Subtract the previous contribution of this feature when moving to the next order\n",
    "            sum_current -= dp[i - 1, j, :]\n",
    "            dp[i, j, :] = (i / (i + 1)) * (X[:,j]* sum_current)\n",
    "            temp_sum += dp[i, j, :]\n",
    "        \n",
    "        sum_current = temp_sum\n",
    "\n",
    "    # Sum up all contributions from the first dimension of each feature to get the final values\n",
    "    omega = torch.sum(dp[:, 0, :], axis=0)\n",
    "\n",
    "    return omega, dp\n",
    "\n",
    "\n",
    "val = torch.zeros(n_features)\n",
    "for i in range(n_features):\n",
    "    omega_dp, _ = Omega(K_per_feature, i, q_additivity=None, feature_type='numerical')\n",
    "    val[i] = torch.matmul(omega_dp, alpha_hat_eta)\n",
    "\n",
    "print('This is val:', val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
