{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: likelihood.noise_covar.raw_noise           value = tensor([3.8435])\n",
      "Parameter name: base_kernel.raw_lengthscale                value = tensor([[-4.1559]])\n",
      "Parameter name: covar_module.raw_outputscale               value = tensor([[20.0976, 23.1188, 14.7172]])\n",
      "Constraint name: likelihood.noise_covar.raw_noise_constraint             constraint = GreaterThan(1.000E-04)\n",
      "Constraint name: base_kernel.raw_lengthscale_constraint                  constraint = Positive()\n",
      "Constraint name: covar_module.raw_outputscale_constraint                 constraint = Positive()\n",
      "Predicted Means:\n",
      "[136.43709  126.317535 128.54861  142.43285  132.39062  128.48618\n",
      " 156.03854  203.0178   149.64705  170.51485  134.30383  150.25023\n",
      " 154.87848  204.96857   99.698975 165.05661  193.5575   196.85657\n",
      " 155.33037  200.43794  159.80981  131.59915  160.64688  205.76811\n",
      " 149.31332  174.03635  217.24796  169.20798  109.80788  126.742065\n",
      " 141.68932  122.119675 166.41295  196.33499  142.92044  166.7475\n",
      "  93.42456  107.39988   75.52655  156.19348  117.278564  88.68889\n",
      "  97.89549  147.75507  112.532196 135.87265   86.896194 178.15717\n",
      " 128.49487  104.33981  131.59723  117.265396 144.8039   130.81226\n",
      " 117.03149  168.11044  121.865326 191.95494  181.29605  107.95204\n",
      " 178.72406  163.7817   153.54984   94.76744  109.84714  229.57837\n",
      " 107.56012  141.48546  150.03409  117.90346  107.29278  189.1025\n",
      " 114.7661   144.98445  129.64163  139.97258  239.52675  158.81061\n",
      " 204.92513  152.97845  134.79019  146.03026  156.30351  158.59195\n",
      "  70.062256 161.41089  142.98514  127.167725 193.1838  ]\n",
      "Predicted Standard Deviations:\n",
      "[ 2.7575903 11.979145  11.199703  12.001477  10.261744  10.175528\n",
      " 11.394894  10.585534  12.343925  11.377492  10.471859  10.216366\n",
      " 11.614831  10.34263   11.430003  10.37837   10.103479  10.931386\n",
      " 10.944889  11.134614  10.706237  10.603678  11.3272705 11.2262\n",
      " 12.13594   10.084178  10.227715  10.433323  11.598163  10.942549\n",
      " 10.458999  11.672399  10.590068  10.351036  11.674357  12.342002\n",
      "  9.879854  10.797668   9.698805  10.424274  10.982922   9.927461\n",
      " 10.068259  11.243146  10.536806  10.549747  10.57966   10.6511965\n",
      " 11.177637  11.686439  10.505967  11.662887  10.694685  10.419653\n",
      " 10.65892   10.635735   9.906837  10.100989  11.039686  10.535529\n",
      " 10.483983  11.202743  11.410525   9.753845   9.770977  10.336784\n",
      " 10.519065  10.497259  11.000925  11.683844  10.787206  10.591111\n",
      " 11.242604  10.959711  11.251471  10.178406   2.7556913 10.55933\n",
      " 10.387033  10.081554  10.760691  10.626649  10.333158  11.092226\n",
      " 10.217805  10.329953  10.498348  11.157201  11.704815 ]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score,train_test_split\n",
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "\n",
    "import torch\n",
    "from linear_operator import to_dense\n",
    "from gpytorch.constraints import Positive\n",
    "from gpytorch.kernels import Kernel\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "X = X[:, :3]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_x, test_x, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 20% data as test\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform it\n",
    "train_x = scaler.fit_transform(train_x)\n",
    "\n",
    "# Transform the test data using the same scaler\n",
    "test_x = scaler.transform(test_x)\n",
    "\n",
    "# Convert to torch tensors\n",
    "train_x = torch.tensor(train_x,dtype=torch.float32)\n",
    "test_x = torch.tensor(test_x, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "k(x_1, x_2) = \\exp\\left(-\\frac{1}{2 \\l^2} \\sum_{k=1}^d \\left(\\left| x_{1k} - x_{2k} \\right|\\right)^2 \\right)\n",
    "'''\n",
    "    \n",
    "class CustomRBFKernel(gpytorch.kernels.Kernel):\n",
    "\n",
    "    has_lengthscale = True\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        # Compute squared distance\n",
    "        # squared_dist = self.covar_dist(x1, x2, square_dist=True, diag=diag, **params)\n",
    "        diff = x1.unsqueeze(1) - x2.unsqueeze(0)\n",
    "        # diff = torch.abs(diff)\n",
    "        squared_dist = (diff ** 2).sum(-1)\n",
    "\n",
    "        # Divide by 2 * lengthscale^2\n",
    "        scaled_squared_dist = squared_dist.div(2 * self.lengthscale.pow(2)) #.div\n",
    "\n",
    "        # Compute exponential\n",
    "        covar_matrix = scaled_squared_dist.mul_(-1).exp_()\n",
    "\n",
    "        return covar_matrix\n",
    "\n",
    "\n",
    "class DPkernel(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, base_kernel, num_dims, q_additivity, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.base_kernel = base_kernel\n",
    "        self.num_dims = num_dims\n",
    "        self.q_additivity = q_additivity\n",
    "        self.register_parameter(\n",
    "            name=\"raw_outputscale\", \n",
    "            parameter=torch.nn.Parameter(torch.zeros(1, self.q_additivity))\n",
    "        )\n",
    "        self.outputscale_constraint = gpytorch.constraints.Positive()\n",
    "        self.register_constraint(\"raw_outputscale\", self.outputscale_constraint)\n",
    "\n",
    "    @property\n",
    "    def outputscale(self):\n",
    "        return self.outputscale_constraint.transform(self.raw_outputscale).squeeze()\n",
    "\n",
    "    @outputscale.setter\n",
    "    def outputscale(self, value):\n",
    "        if not torch.is_tensor(value):\n",
    "            value = torch.tensor(value, device=self.raw_outputscale.device)\n",
    "        self.initialize(raw_outputscale=self.outputscale_constraint.inverse_transform(value))\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "    # Determine sizes based on input matrices\n",
    "        x1_size = x1.size(0)\n",
    "        x2_size = x2.size(0)\n",
    "        \n",
    "        # Initialize matrices based on input sizes\n",
    "        result = torch.zeros(x1_size, x2_size, device=x1.device) #initialize the result matrix\n",
    "        sum_order_b = torch.zeros(x1_size, x2_size, device=x1.device) # initialize the matrix for the matrix for a single order\n",
    "        kernels =[] # list were the z1, z2,... would be stored\n",
    "\n",
    "        # print(f\"Initial x1 shape: {x1.shape}, x2 shape: {x2.shape}\")\n",
    "        \n",
    "        #calculations for first order\n",
    "        #calcualte the kernels for each dimentions\n",
    "        for d in range(self.num_dims):\n",
    "            x1_d = x1[:, d:d+1]\n",
    "            x2_d = x2[:, d:d+1]\n",
    "            k_d = self.base_kernel(x1_d, x2_d).evaluate() # change thek to k0\n",
    "            kernels.append(k_d) #save them in order in the kernels list\n",
    "            # print(f\"Kernel k_d at dim {d} shape: {k_d.shape}, sum_order_b shape: {sum_order_b.shape}\")\n",
    "\n",
    "            sum_order_b += k_d # add each one dimension kernels to one matrix for first order\n",
    "    \n",
    "        # first_kernels = kernels\n",
    "        outputscale = self.outputscale.unsqueeze(0) if len(self.outputscale.shape) == 0 else self.outputscale\n",
    "        result += sum_order_b * self.outputscale[0] #add the first order kernel miltiplied by first outputscale\n",
    "\n",
    "        # Compute higher order interactions\n",
    "        for i in range(1, self.q_additivity):\n",
    "            temp_sum = torch.zeros(x1_size, x2_size, device=x1.device)\n",
    "            new_kernels = []\n",
    "            for j in range(self.num_dims):\n",
    "                for k in range(j + 1, self.num_dims):\n",
    "                    new_kernel = kernels[j] * kernels[k]\n",
    "                    new_kernels.append(new_kernel)\n",
    "                    temp_sum += new_kernel\n",
    "\n",
    "            kernels = new_kernels  # update kernels list with new order interactions\n",
    "            result += temp_sum * self.outputscale[i]\n",
    "\n",
    "        return result\n",
    "\n",
    "# Example usage in a GP model\n",
    "class MyGP(gpytorch.models.ExactGP): # i need to find a diferent model\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(MyGP, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        # self.base_kernel = gpytorch.kernels.RBFKernel()\n",
    "        self.base_kernel = CustomRBFKernel()\n",
    "        self.covar_module = DPkernel(base_kernel=self.base_kernel, num_dims=train_x.size(-1), q_additivity=train_x.size(-1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x,x)  # Make sure to pass x twice WHY\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Create the GP model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "\n",
    "model = MyGP(train_x, y_train.squeeze(-1), likelihood)\n",
    "model.eval()\n",
    "# with torch.no_grad():\n",
    "#     untrained_pred_dist = likelihood(model(test_x))\n",
    "#     predictive_mean = untrained_pred_dist.mean\n",
    "#     lower, upper = untrained_pred_dist.confidence_region()\n",
    "# Set up optimizer and marginal log likelihood\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "# Training loop\n",
    "training_iter = 1000\n",
    "for i in range(training_iter):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    # print(output)\n",
    "    loss = -mll(output, y_train)\n",
    "    loss = loss.mean() \n",
    "    loss.backward()\n",
    "    # print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "    #     i + 1, training_iter, loss.item(),\n",
    "    #     model.covar_module.base_kernel.lengthscale.item(),\n",
    "    #     model.likelihood.noise.item()\n",
    "    # ))\n",
    "    optimizer.step()\n",
    "# print('likelihood noise', likelihood.noise)\n",
    "# print('likelihood noise raw', likelihood.noise_covar.raw_noise)\n",
    "model.eval()\n",
    "# with torch.no_grad():\n",
    "#     trained_pred_dist = likelihood(model(test_x))\n",
    "#     predictive_mean = trained_pred_dist.mean\n",
    "#     lower, upper = trained_pred_dist.confidence_region()\n",
    "# Viewing model parameters after training\n",
    "for param_name, param in model.named_parameters():\n",
    "    print(f'Parameter name: {param_name:42} value = {param.data}')\n",
    "\n",
    "\n",
    "#evaluating there is a problem when the test_y and test_x have float numbers\n",
    "with torch.no_grad():\n",
    "    model.eval()  # Set the model to evaluation mode (mode is for computing predictions through the model posterior.)\n",
    "    likelihood.eval()\n",
    "    output = likelihood(model(test_x))  # Make predictions on new data \n",
    "    \n",
    "\n",
    "\n",
    "for constraint_name, constraint in model.named_constraints():\n",
    "    print(f'Constraint name: {constraint_name:55} constraint = {constraint}')\n",
    "\n",
    "\n",
    "# Extracting means and standard deviations\n",
    "predicted_means = output.mean.numpy() \n",
    "predicted_stddevs = output.stddev.numpy()  # Extract standard deviations\n",
    "\n",
    "print(\"Predicted Means:\")\n",
    "print(predicted_means)\n",
    "\n",
    "print(\"Predicted Standard Deviations:\")\n",
    "print(predicted_stddevs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate teh alpha_hat_eta\n",
    "with torch.no_grad():\n",
    "    # Evaluate the kernel matrix\n",
    "    t_k_matrix = model.covar_module(train_x).evaluate()\n",
    "    \n",
    "    # Ensure the noise variance is non-zero and sufficiently large to avoid singularity\n",
    "    noise_variance = likelihood.noise_covar.noise if likelihood.noise_covar.noise > 1e-6 else 1e-6\n",
    "    n_matrix = torch.eye(t_k_matrix.size(-1), device=t_k_matrix.device) * noise_variance\n",
    "    \n",
    "    # Add regularization to avoid singular matrix\n",
    "\n",
    "    K_inv = torch.inverse(t_k_matrix + n_matrix + torch.eye(t_k_matrix.size(-1), device=t_k_matrix.device))\n",
    "\n",
    "    # Compute alpha_hat_eta using the inverse (dot product)\n",
    "    alpha_hat_eta = torch.matmul(K_inv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 65.5050, 115.2710,  64.4729],\n",
      "        [ 66.1310, 115.2710,  64.0486],\n",
      "        [ 66.7570, 121.8438,  64.2640],\n",
      "        ...,\n",
      "        [ 66.4440, 121.8438,  64.8722],\n",
      "        [ 64.2530, 121.8438,  66.0921],\n",
      "        [ 64.2530, 121.8438,  64.0591]])\n"
     ]
    }
   ],
   "source": [
    "# print(K.shape)  # Should be (n_samples, n_features)\n",
    "n_samples, n_features = train_x.shape\n",
    "\n",
    "# Initialize a tensor to hold the full kernel matrix K\n",
    "K = torch.zeros(n_samples, n_features)\n",
    "\n",
    "with torch.no_grad():\n",
    "    kernel = model.covar_module\n",
    "\n",
    "    for j in range(n_features):\n",
    "        # Extract the j-th feature from all samples, ensure it is 2D: [n_samples, 1]\n",
    "        x_feature_j = train_x[:, j].unsqueeze(1)\n",
    "        \n",
    "        # Compute the kernel matrix for this feature across all samples\n",
    "        # This assumes the kernel can process a matrix of shape [n_samples, 1]\n",
    "        # and output a distance matrix of shape [n_samples, n_samples]\n",
    "        pred_dist_matrix = kernel( x_feature_j).evaluate()\n",
    "\n",
    "        # Since we want to store only specific values or summarization, let's store the mean of each row\n",
    "        # This operation depends on your specific requirement, here we assume mean for example purposes\n",
    "        K[:, j] = pred_dist_matrix.mean(dim=1)\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is shapley value tensor([1.4436e+25, 1.4436e+25, 1.4436e+25])\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "n_samples, n_features = train_x.size()\n",
    "val = torch.zeros(n_features)\n",
    "#### calculate the extendent K\n",
    "\n",
    "\n",
    "# Generate all combinations of feature indices\n",
    "feature_combinations = list(itertools.chain.from_iterable(\n",
    "    itertools.combinations(range(n_features), r) for r in range(1, n_features + 1)\n",
    "))\n",
    "\n",
    "# Initialize an extended kernel matrix to store the results\n",
    "extended_K = torch.zeros((n_samples, len(feature_combinations)))\n",
    "\n",
    "# Compute the product of kernels for each combination\n",
    "for idx, combination in enumerate(feature_combinations):\n",
    "    # print(type(combination[0]))\n",
    "    # Start with the first column in the combination\n",
    "    product_kernel = K[:, combination[0]]\n",
    "    if len(combination)>1:\n",
    "        # Multiply by each subsequent column in the combination\n",
    "        for col in combination[1:]:\n",
    "            product_kernel *= K[:, col]\n",
    "    \n",
    "    # Store the result in the corresponding column of extended_K\n",
    "    extended_K[:, idx] = product_kernel\n",
    "\n",
    "# print(extended_K)\n",
    "\n",
    "# print(\"THIS IS EXTENDED\", extended_K)\n",
    "## \n",
    "\n",
    "# create a loop for each feature\n",
    "for j in range(n_features):\n",
    "    \n",
    "    # find the subsets where the j-th feature was used to create (find the index)\n",
    "    # print('pre', indices_of_kj_columns)\n",
    "    indices_of_kj_columns = []\n",
    "    \n",
    "        # Check if kj is present in the combination\n",
    "    for idx, combination in enumerate(feature_combinations):\n",
    "        # print(idx,combination)\n",
    "        if j in combination:\n",
    "            indices_of_kj_columns.append(idx)\n",
    "    indices_of_kj_columns = torch.tensor(indices_of_kj_columns) # change it from a list to a tensor\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "    ### updated extended K\n",
    "    updated_extended_K = torch.zeros(n_samples, len(indices_of_kj_columns)) # initialise\n",
    "    # update the K_extended matrix so that only the columns with the indexes found \n",
    "    # Copy the columns from extended_K corresponding to indices_of_kj_columns\n",
    "    for i, idx in enumerate(indices_of_kj_columns):\n",
    "        # idx = int(idx)\n",
    "        updated_extended_K[:, i] = extended_K[:, idx]\n",
    "  \n",
    "\n",
    "    #create a vector of weights for the corresponding columns (a weight i sthe 1/len(number of indences) that build this column) \n",
    "    weights = torch.zeros((len(indices_of_kj_columns))).unsqueeze(-1)\n",
    "\n",
    "    #Set the diagonal elements to the number of kernels needed to cover each feature combination\n",
    "    for i, idx in enumerate(indices_of_kj_columns):\n",
    "        weights[i] = 1/len(feature_combinations[idx])\n",
    "    # print('weights', weights)\n",
    "    omega = torch.matmul( updated_extended_K, weights)\n",
    "    \n",
    "    # Transpose the result back to its original shape\n",
    "    omega = omega.T.squeeze(-1)\n",
    "    \n",
    "\n",
    "    val[j] = torch.matmul(omega, alpha_hat_eta)\n",
    "    \n",
    "\n",
    "\n",
    "print('this is shapley value',val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
