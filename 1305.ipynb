{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: likelihood.noise_covar.raw_noise           value = tensor([-0.8754])\n",
      "Parameter name: base_kernel.raw_lengthscale                value = tensor([[2.4317]])\n",
      "Parameter name: covar_module.raw_outputscale               value = tensor([[ 1.5483, -6.6685, -7.0663]])\n",
      "Constraint name: likelihood.noise_covar.raw_noise_constraint             constraint = GreaterThan(1.000E-04)\n",
      "Constraint name: base_kernel.raw_lengthscale_constraint                  constraint = Positive()\n",
      "Constraint name: covar_module.raw_outputscale_constraint                 constraint = Positive()\n",
      "Predicted Means:\n",
      "[1.9209824  1.4819489  1.2753677  3.7873154  3.770172   1.9654617\n",
      " 0.9441147  1.9581833  1.5320206  1.7387543  0.90864563 1.41539\n",
      " 1.7202759  0.92178345 1.8711548  1.8441696  1.1222763  1.968689\n",
      " 1.2792435  1.3386154  3.5848465  1.6737366  1.812088   2.026825\n",
      " 0.93388367 1.9927444  1.2899933  1.704628   3.0830154  3.6203918\n",
      " 2.1685486  1.2870789  1.1572113  1.3131256  2.3348312  2.1416626\n",
      " 3.9047241  3.7842712  1.9478836  1.5835876 ]\n",
      "Predicted Standard Deviations:\n",
      "[0.62146133 0.5941084  0.5963812  0.6199388  0.61259544 0.5959147\n",
      " 0.60327303 0.60112065 0.59475255 0.5950383  0.6047498  0.5947611\n",
      " 0.6000258  0.62368023 0.59555334 0.5970473  0.5981234  0.5958794\n",
      " 0.596876   0.6203925  0.605802   0.60112476 0.5959013  0.6029801\n",
      " 0.6040758  0.6006946  0.60739934 0.6195105  0.6082569  0.6115231\n",
      " 0.59639424 0.5974591  0.59779423 0.6145143  0.5992629  0.6036313\n",
      " 0.6190732  0.6184474  0.59844637 0.60656124]\n",
      "x_train torch.Size([160, 3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score,train_test_split\n",
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "\n",
    "import torch\n",
    "from linear_operator import to_dense\n",
    "from gpytorch.constraints import Positive\n",
    "from gpytorch.kernels import Kernel\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "X = X[:200, :]\n",
    "y = y[:200]\n",
    "X = X[:, :3]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_x, test_x, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 20% data as test\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform it\n",
    "train_x = scaler.fit_transform(train_x)\n",
    "\n",
    "# Transform the test data using the same scaler\n",
    "test_x = scaler.transform(test_x)\n",
    "\n",
    "# Convert to torch tensors\n",
    "train_x = torch.tensor(train_x,dtype=torch.float32)\n",
    "test_x = torch.tensor(test_x, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "k(x_1, x_2) = \\exp\\left(-\\frac{1}{2 \\l^2} \\sum_{k=1}^d \\left(\\left| x_{1k} - x_{2k} \\right|\\right)^2 \\right)\n",
    "'''\n",
    "    \n",
    "class CustomRBFKernel(gpytorch.kernels.Kernel):\n",
    "\n",
    "    has_lengthscale = True\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        # Compute squared distance\n",
    "        # squared_dist = self.covar_dist(x1, x2, square_dist=True, diag=diag, **params)\n",
    "        diff = x1.unsqueeze(1) - x2.unsqueeze(0)\n",
    "        diff = torch.abs(diff)\n",
    "        squared_dist = (diff ** 2).sum(-1)\n",
    "\n",
    "        # Divide by 2 * lengthscale^2\n",
    "        scaled_squared_dist = squared_dist.div(2 * self.lengthscale.pow(2)) #.div\n",
    "\n",
    "        # Compute exponential\n",
    "        covar_matrix = scaled_squared_dist.mul_(-1).exp_()\n",
    "\n",
    "        return covar_matrix\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "class DPkernel(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, base_kernel, num_dims, q_additivity, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.base_kernel = base_kernel\n",
    "        self.num_dims = num_dims\n",
    "        self.q_additivity = q_additivity\n",
    "        self.register_parameter(\n",
    "            name=\"raw_outputscale\", \n",
    "            parameter=torch.nn.Parameter(torch.zeros(1, self.q_additivity))\n",
    "        )\n",
    "        self.outputscale_constraint = gpytorch.constraints.Positive()\n",
    "        self.register_constraint(\"raw_outputscale\", self.outputscale_constraint)\n",
    "\n",
    "    @property\n",
    "    def outputscale(self):\n",
    "        return self.outputscale_constraint.transform(self.raw_outputscale).squeeze()\n",
    "\n",
    "    @outputscale.setter\n",
    "    def outputscale(self, value):\n",
    "        if not torch.is_tensor(value):\n",
    "            value = torch.tensor(value, device=self.raw_outputscale.device)\n",
    "        self.initialize(raw_outputscale=self.outputscale_constraint.inverse_transform(value))\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "    # Determine sizes based on input matrices\n",
    "        x1_size = x1.size(0)\n",
    "        x2_size = x2.size(0)\n",
    "        \n",
    "        # Initialize matrices based on input sizes\n",
    "        result = torch.zeros(x1_size, x2_size, device=x1.device) #initialize the result matrix\n",
    "        sum_order_b = torch.zeros(x1_size, x2_size, device=x1.device) # initialize the matrix for the matrix for a single order\n",
    "        kernels =[] # list were the z1, z2,... would be stored\n",
    "\n",
    "        # print(f\"Initial x1 shape: {x1.shape}, x2 shape: {x2.shape}\")\n",
    "        \n",
    "        #calculations for first order\n",
    "        #calcualte the kernels for each dimentions\n",
    "        for d in range(self.num_dims):\n",
    "            x1_d = x1[:, d:d+1]\n",
    "            x2_d = x2[:, d:d+1]\n",
    "            k_d = self.base_kernel(x1_d, x2_d).evaluate() # change thek to k0\n",
    "            \n",
    "            kernels.append(k_d) #save them in order in the kernels list\n",
    "            # print(f\"Kernel k_d at dim {d} shape: {k_d.shape}, sum_order_b shape: {sum_order_b.shape}\")\n",
    "\n",
    "            sum_order_b += k_d # add each one dimension kernels to one matrix for first order\n",
    "    \n",
    "        # first_kernels = kernels\n",
    "        outputscale = self.outputscale.unsqueeze(0) if len(self.outputscale.shape) == 0 else self.outputscale\n",
    "        result += sum_order_b * self.outputscale[0] #add the first order kernel miltiplied by first outputscale\n",
    "\n",
    "        # Compute higher order interactions\n",
    "        for i in range(1, self.q_additivity):\n",
    "            temp_sum = torch.zeros(x1_size, x2_size, device=x1.device)\n",
    "            new_kernels = []\n",
    "            for j in range(self.num_dims):\n",
    "                for k in range(j + 1, self.num_dims):\n",
    "                    new_kernel = kernels[j] * kernels[k]\n",
    "                    new_kernels.append(new_kernel)\n",
    "                    temp_sum += new_kernel\n",
    "\n",
    "            kernels = new_kernels  # update kernels list with new order interactions\n",
    "            result += temp_sum * self.outputscale[i]\n",
    "\n",
    "        return result\n",
    "\n",
    "# Example usage in a GP model\n",
    "class MyGP(gpytorch.models.ExactGP): # i need to find a diferent model\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(MyGP, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        # self.base_kernel = gpytorch.kernels.RBFKernel()\n",
    "        self.base_kernel = CustomRBFKernel()\n",
    "        self.covar_module = DPkernel(base_kernel=self.base_kernel, num_dims=train_x.size(-1), q_additivity=train_x.size(-1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x,x)  # Make sure to pass x twice WHY\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Create the GP model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "\n",
    "model = MyGP(train_x, y_train.squeeze(-1), likelihood)\n",
    "model.eval()\n",
    "# with torch.no_grad():\n",
    "#     untrained_pred_dist = likelihood(model(test_x))\n",
    "#     predictive_mean = untrained_pred_dist.mean\n",
    "#     lower, upper = untrained_pred_dist.confidence_region()\n",
    "# Set up optimizer and marginal log likelihood\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "# Training loop\n",
    "training_iter = 1000\n",
    "for i in range(training_iter):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    # print(output)\n",
    "    loss = -mll(output, y_train)\n",
    "    loss = loss.mean() \n",
    "    loss.backward()\n",
    "    # print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "    #     i + 1, training_iter, loss.item(),\n",
    "    #     model.covar_module.base_kernel.lengthscale.item(),\n",
    "    #     model.likelihood.noise.item()\n",
    "    # ))\n",
    "    optimizer.step()\n",
    "# print('likelihood noise', likelihood.noise)\n",
    "# print('likelihood noise raw', likelihood.noise_covar.raw_noise)\n",
    "model.eval()\n",
    "# with torch.no_grad():\n",
    "#     trained_pred_dist = likelihood(model(test_x))\n",
    "#     predictive_mean = trained_pred_dist.mean\n",
    "#     lower, upper = trained_pred_dist.confidence_region()\n",
    "# Viewing model parameters after training\n",
    "for param_name, param in model.named_parameters():\n",
    "    print(f'Parameter name: {param_name:42} value = {param.data}')\n",
    "\n",
    "\n",
    "#evaluating there is a problem when the test_y and test_x have float numbers\n",
    "with torch.no_grad():\n",
    "    model.eval()  # Set the model to evaluation mode (mode is for computing predictions through the model posterior.)\n",
    "    likelihood.eval()\n",
    "    output = likelihood(model(test_x))  # Make predictions on new data \n",
    "    \n",
    "\n",
    "\n",
    "for constraint_name, constraint in model.named_constraints():\n",
    "    print(f'Constraint name: {constraint_name:55} constraint = {constraint}')\n",
    "\n",
    "\n",
    "# Extracting means and standard deviations\n",
    "predicted_means = output.mean.numpy() \n",
    "predicted_stddevs = output.stddev.numpy()  # Extract standard deviations\n",
    "\n",
    "print(\"Predicted Means:\")\n",
    "print(predicted_means)\n",
    "\n",
    "print(\"Predicted Standard Deviations:\")\n",
    "print(predicted_stddevs)\n",
    "\n",
    "\n",
    "print(\"x_train\", train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([160, 1])\n"
     ]
    }
   ],
   "source": [
    "# calculate teh alpha_hat_eta\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad():\n",
    "    # Evaluate the kernel matrix\n",
    "    t_k_matrix = model.covar_module(train_x).evaluate()\n",
    "    \n",
    "    # Ensure the noise variance is non-zero and sufficiently large to avoid singularity\n",
    "    noise_variance = likelihood.noise_covar.noise if likelihood.noise_covar.noise > 1e-6 else 1e-6\n",
    "    n_matrix = torch.eye(t_k_matrix.size(-1), device=t_k_matrix.device) * noise_variance\n",
    "    \n",
    "    # Add regularization to avoid singular matrix\n",
    "\n",
    "    K_inv = torch.inverse(t_k_matrix + n_matrix)# + torch.eye(t_k_matrix.size(-1), device=t_k_matrix.device))\n",
    "\n",
    "    # Compute alpha_hat_eta using the inverse (dot product)\n",
    "    alpha_hat_eta = torch.matmul(K_inv, y_train).unsqueeze(1)\n",
    "\n",
    "    print(alpha_hat_eta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.2182, 5.2295, 5.2045, 5.2295, 4.8285, 5.2029, 5.2026, 5.2295, 5.2267,\n",
      "        5.1459, 5.2049, 5.2004, 5.1636, 4.6000, 5.2217, 5.0918, 4.8249, 5.1685,\n",
      "        5.2250, 5.2277, 5.2076, 5.2180, 5.2163, 5.2221, 5.1896, 5.0099, 5.1506,\n",
      "        5.2221, 5.2212, 4.7300, 5.1909, 4.8879, 5.1923, 5.1742, 4.9160, 5.2197,\n",
      "        5.2288, 4.8309, 5.2291, 5.2294, 4.6493, 5.1589, 5.2188, 5.2129, 5.2256,\n",
      "        5.2283, 5.2239, 5.1576, 5.2100, 4.8700, 5.2048, 4.6900, 5.2171, 5.1814,\n",
      "        5.2149, 5.1956, 5.2288, 5.1934, 5.2092, 4.8991, 5.1797, 5.2086, 4.7436,\n",
      "        5.0169, 5.1749, 5.2220, 5.2177, 5.1381, 5.2237, 4.7947, 5.1980, 4.4318,\n",
      "        5.2294, 5.0828, 5.2073, 5.0263, 5.2295, 5.2239, 5.2168, 5.1999, 5.2213,\n",
      "        5.1754, 4.3214, 4.7290, 5.2282, 5.2238, 5.2281, 5.2152, 5.1639, 5.2295,\n",
      "        5.2269, 5.2053, 5.2112, 4.4661, 5.1700, 4.6975, 4.9529, 5.0615, 5.1710,\n",
      "        4.7214, 5.2293, 5.2279, 5.2241, 5.2075, 5.1906, 5.2289, 5.2207, 5.0388,\n",
      "        5.1345, 5.1281, 5.2224, 5.2153, 5.1706, 5.2207, 5.2293, 5.2294, 5.2238,\n",
      "        3.8697, 5.2220, 5.2061, 5.2231, 4.4562, 5.1827, 5.2255, 5.1661, 5.1979,\n",
      "        5.2190, 5.1403, 5.2002, 5.1684, 5.1432, 5.1454, 5.2041, 5.1291, 5.2241,\n",
      "        5.2214, 4.8794, 5.1840, 4.8859, 5.1973, 4.4367, 5.1588, 5.1110, 4.6901,\n",
      "        5.0000, 5.2295, 5.2295, 5.2285, 5.1228, 5.2290, 5.0569, 5.2294, 5.1800,\n",
      "        5.2233, 5.1192, 5.2148, 5.2144, 5.1217, 5.2253, 5.2205])\n",
      "tensor([5.0215, 5.2295, 5.2295, 5.2295, 5.2295, 4.3837, 5.2295, 5.2295, 5.2250,\n",
      "        5.0742, 4.9317, 5.1403, 5.2295, 5.2250, 5.2295, 5.1893, 4.7601, 4.4589,\n",
      "        4.9630, 4.7961, 5.1750, 4.7235, 5.2295, 5.2295, 5.2193, 5.2015, 5.2193,\n",
      "        5.2015, 4.2385, 4.7235, 5.1403, 5.1587, 5.2295, 5.2283, 5.2250, 5.2295,\n",
      "        5.2193, 5.0486, 4.7235, 4.8314, 5.2295, 4.3467, 5.2250, 4.7601, 5.1403,\n",
      "        5.2295, 5.2295, 5.2115, 5.2295, 5.2193, 5.2295, 5.2295, 4.6863, 5.2193,\n",
      "        5.2295, 5.2295, 5.2295, 5.2295, 5.2295, 5.0980, 5.2295, 5.2115, 5.1750,\n",
      "        5.2295, 4.6863, 5.0215, 5.2295, 5.2250, 4.6110, 5.2295, 5.0980, 5.0980,\n",
      "        5.2295, 5.2295, 4.6110, 5.2295, 5.2115, 5.2295, 5.1893, 5.2250, 5.2295,\n",
      "        5.2295, 5.2295, 5.2295, 5.0980, 5.2295, 5.2295, 5.2295, 5.2295, 5.2295,\n",
      "        4.6488, 5.1403, 5.0486, 5.2295, 5.2295, 4.9930, 5.2295, 5.2295, 5.2295,\n",
      "        4.8993, 5.2193, 5.2295, 5.1403, 5.0742, 4.2740, 5.2283, 5.2295, 5.2295,\n",
      "        5.2295, 5.2295, 5.2295, 5.2295, 5.2295, 5.1201, 5.2295, 3.8300, 5.2295,\n",
      "        4.3101, 5.2295, 5.2193, 5.2295, 4.6863, 4.7235, 5.2250, 5.2295, 5.2295,\n",
      "        5.2295, 4.4212, 5.2295, 5.2295, 5.0742, 4.0413, 5.1201, 5.2295, 5.2295,\n",
      "        5.2283, 4.7601, 5.2295, 5.2295, 5.0486, 4.4212, 5.0980, 5.2295, 4.8993,\n",
      "        5.2295, 5.2295, 4.9930, 4.6863, 4.0413, 4.3837, 4.9930, 5.2295, 5.0742,\n",
      "        5.0486, 5.1893, 5.2295, 5.2295, 5.2295, 5.2295, 5.0486])\n",
      "tensor([5.1766, 5.1737, 5.2103, 5.2295, 5.0354, 5.0172, 5.2061, 5.2267, 5.2281,\n",
      "        4.6951, 5.0828, 5.0456, 5.2107, 5.1766, 5.1224, 4.5119, 5.2208, 4.8760,\n",
      "        4.9145, 5.0966, 5.0943, 5.0544, 5.2185, 5.2293, 5.1972, 5.1144, 5.0730,\n",
      "        5.1897, 4.8207, 5.0942, 5.1959, 5.2137, 5.2167, 5.0703, 5.0780, 5.1948,\n",
      "        5.2081, 4.8907, 5.0634, 5.0638, 4.6759, 5.2066, 5.0584, 5.0781, 5.1924,\n",
      "        5.0820, 5.1588, 5.2139, 5.1175, 5.2034, 5.2273, 5.0694, 5.0935, 5.2253,\n",
      "        5.2115, 5.1108, 5.1665, 5.2211, 5.2121, 5.0560, 5.1562, 5.2136, 5.0332,\n",
      "        5.1673, 5.1418, 5.1996, 5.2263, 5.1635, 4.9560, 5.1387, 5.0286, 5.0274,\n",
      "        5.1481, 5.0809, 5.1859, 5.1450, 5.1640, 5.2216, 5.1656, 5.1752, 5.1500,\n",
      "        5.2165, 4.4670, 4.8747, 5.1862, 5.2290, 5.1835, 5.2095, 5.0685, 5.0135,\n",
      "        5.0792, 5.1120, 5.0525, 4.8464, 5.2132, 5.0277, 5.2077, 5.1694, 5.1281,\n",
      "        5.1026, 5.1465, 5.2005, 5.2162, 4.8041, 4.9420, 5.2230, 5.2132, 5.2201,\n",
      "        4.7330, 4.7469, 5.2086, 5.1492, 4.8716, 5.1612, 5.1154, 4.8280, 5.0559,\n",
      "        4.6618, 5.1244, 5.2236, 5.1955, 5.0286, 4.9409, 5.1488, 4.8476, 5.2250,\n",
      "        5.1426, 4.6133, 4.8452, 5.1932, 5.0975, 4.8055, 5.1842, 5.1796, 5.2267,\n",
      "        4.9381, 5.1313, 5.0683, 5.2075, 5.0681, 5.1623, 5.1837, 5.2295, 5.1769,\n",
      "        5.2189, 5.0183, 5.0597, 4.8969, 4.7938, 5.1567, 4.9996, 5.0677, 5.1893,\n",
      "        5.0737, 4.9858, 5.1802, 5.1569, 4.8424, 5.1862, 5.0676])\n",
      "Matrix K (n*d):\n",
      "tensor([[5.2182, 5.0215, 5.1766],\n",
      "        [5.2295, 5.2295, 5.1737],\n",
      "        [5.2045, 5.2295, 5.2103],\n",
      "        [5.2295, 5.2295, 5.2295],\n",
      "        [4.8285, 5.2295, 5.0354],\n",
      "        [5.2029, 4.3837, 5.0172],\n",
      "        [5.2026, 5.2295, 5.2061],\n",
      "        [5.2295, 5.2295, 5.2267],\n",
      "        [5.2267, 5.2250, 5.2281],\n",
      "        [5.1459, 5.0742, 4.6951],\n",
      "        [5.2049, 4.9317, 5.0828],\n",
      "        [5.2004, 5.1403, 5.0456],\n",
      "        [5.1636, 5.2295, 5.2107],\n",
      "        [4.6000, 5.2250, 5.1766],\n",
      "        [5.2217, 5.2295, 5.1224],\n",
      "        [5.0918, 5.1893, 4.5119],\n",
      "        [4.8249, 4.7601, 5.2208],\n",
      "        [5.1685, 4.4589, 4.8760],\n",
      "        [5.2250, 4.9630, 4.9145],\n",
      "        [5.2277, 4.7961, 5.0966],\n",
      "        [5.2076, 5.1750, 5.0943],\n",
      "        [5.2180, 4.7235, 5.0544],\n",
      "        [5.2163, 5.2295, 5.2185],\n",
      "        [5.2221, 5.2295, 5.2293],\n",
      "        [5.1896, 5.2193, 5.1972],\n",
      "        [5.0099, 5.2015, 5.1144],\n",
      "        [5.1506, 5.2193, 5.0730],\n",
      "        [5.2221, 5.2015, 5.1897],\n",
      "        [5.2212, 4.2385, 4.8207],\n",
      "        [4.7300, 4.7235, 5.0942],\n",
      "        [5.1909, 5.1403, 5.1959],\n",
      "        [4.8879, 5.1587, 5.2137],\n",
      "        [5.1923, 5.2295, 5.2167],\n",
      "        [5.1742, 5.2283, 5.0703],\n",
      "        [4.9160, 5.2250, 5.0780],\n",
      "        [5.2197, 5.2295, 5.1948],\n",
      "        [5.2288, 5.2193, 5.2081],\n",
      "        [4.8309, 5.0486, 4.8907],\n",
      "        [5.2291, 4.7235, 5.0634],\n",
      "        [5.2294, 4.8314, 5.0638],\n",
      "        [4.6493, 5.2295, 4.6759],\n",
      "        [5.1589, 4.3467, 5.2066],\n",
      "        [5.2188, 5.2250, 5.0584],\n",
      "        [5.2129, 4.7601, 5.0781],\n",
      "        [5.2256, 5.1403, 5.1924],\n",
      "        [5.2283, 5.2295, 5.0820],\n",
      "        [5.2239, 5.2295, 5.1588],\n",
      "        [5.1576, 5.2115, 5.2139],\n",
      "        [5.2100, 5.2295, 5.1175],\n",
      "        [4.8700, 5.2193, 5.2034],\n",
      "        [5.2048, 5.2295, 5.2273],\n",
      "        [4.6900, 5.2295, 5.0694],\n",
      "        [5.2171, 4.6863, 5.0935],\n",
      "        [5.1814, 5.2193, 5.2253],\n",
      "        [5.2149, 5.2295, 5.2115],\n",
      "        [5.1956, 5.2295, 5.1108],\n",
      "        [5.2288, 5.2295, 5.1665],\n",
      "        [5.1934, 5.2295, 5.2211],\n",
      "        [5.2092, 5.2295, 5.2121],\n",
      "        [4.8991, 5.0980, 5.0560],\n",
      "        [5.1797, 5.2295, 5.1562],\n",
      "        [5.2086, 5.2115, 5.2136],\n",
      "        [4.7436, 5.1750, 5.0332],\n",
      "        [5.0169, 5.2295, 5.1673],\n",
      "        [5.1749, 4.6863, 5.1418],\n",
      "        [5.2220, 5.0215, 5.1996],\n",
      "        [5.2177, 5.2295, 5.2263],\n",
      "        [5.1381, 5.2250, 5.1635],\n",
      "        [5.2237, 4.6110, 4.9560],\n",
      "        [4.7947, 5.2295, 5.1387],\n",
      "        [5.1980, 5.0980, 5.0286],\n",
      "        [4.4318, 5.0980, 5.0274],\n",
      "        [5.2294, 5.2295, 5.1481],\n",
      "        [5.0828, 5.2295, 5.0809],\n",
      "        [5.2073, 4.6110, 5.1859],\n",
      "        [5.0263, 5.2295, 5.1450],\n",
      "        [5.2295, 5.2115, 5.1640],\n",
      "        [5.2239, 5.2295, 5.2216],\n",
      "        [5.2168, 5.1893, 5.1656],\n",
      "        [5.1999, 5.2250, 5.1752],\n",
      "        [5.2213, 5.2295, 5.1500],\n",
      "        [5.1754, 5.2295, 5.2165],\n",
      "        [4.3214, 5.2295, 4.4670],\n",
      "        [4.7290, 5.2295, 4.8747],\n",
      "        [5.2282, 5.0980, 5.1862],\n",
      "        [5.2238, 5.2295, 5.2290],\n",
      "        [5.2281, 5.2295, 5.1835],\n",
      "        [5.2152, 5.2295, 5.2095],\n",
      "        [5.1639, 5.2295, 5.0685],\n",
      "        [5.2295, 5.2295, 5.0135],\n",
      "        [5.2269, 4.6488, 5.0792],\n",
      "        [5.2053, 5.1403, 5.1120],\n",
      "        [5.2112, 5.0486, 5.0525],\n",
      "        [4.4661, 5.2295, 4.8464],\n",
      "        [5.1700, 5.2295, 5.2132],\n",
      "        [4.6975, 4.9930, 5.0277],\n",
      "        [4.9529, 5.2295, 5.2077],\n",
      "        [5.0615, 5.2295, 5.1694],\n",
      "        [5.1710, 5.2295, 5.1281],\n",
      "        [4.7214, 4.8993, 5.1026],\n",
      "        [5.2293, 5.2193, 5.1465],\n",
      "        [5.2279, 5.2295, 5.2005],\n",
      "        [5.2241, 5.1403, 5.2162],\n",
      "        [5.2075, 5.0742, 4.8041],\n",
      "        [5.1906, 4.2740, 4.9420],\n",
      "        [5.2289, 5.2283, 5.2230],\n",
      "        [5.2207, 5.2295, 5.2132],\n",
      "        [5.0388, 5.2295, 5.2201],\n",
      "        [5.1345, 5.2295, 4.7330],\n",
      "        [5.1281, 5.2295, 4.7469],\n",
      "        [5.2224, 5.2295, 5.2086],\n",
      "        [5.2153, 5.2295, 5.1492],\n",
      "        [5.1706, 5.2295, 4.8716],\n",
      "        [5.2207, 5.1201, 5.1612],\n",
      "        [5.2293, 5.2295, 5.1154],\n",
      "        [5.2294, 3.8300, 4.8280],\n",
      "        [5.2238, 5.2295, 5.0559],\n",
      "        [3.8697, 4.3101, 4.6618],\n",
      "        [5.2220, 5.2295, 5.1244],\n",
      "        [5.2061, 5.2193, 5.2236],\n",
      "        [5.2231, 5.2295, 5.1955],\n",
      "        [4.4562, 4.6863, 5.0286],\n",
      "        [5.1827, 4.7235, 4.9409],\n",
      "        [5.2255, 5.2250, 5.1488],\n",
      "        [5.1661, 5.2295, 4.8476],\n",
      "        [5.1979, 5.2295, 5.2250],\n",
      "        [5.2190, 5.2295, 5.1426],\n",
      "        [5.1403, 4.4212, 4.6133],\n",
      "        [5.2002, 5.2295, 4.8452],\n",
      "        [5.1684, 5.2295, 5.1932],\n",
      "        [5.1432, 5.0742, 5.0975],\n",
      "        [5.1454, 4.0413, 4.8055],\n",
      "        [5.2041, 5.1201, 5.1842],\n",
      "        [5.1291, 5.2295, 5.1796],\n",
      "        [5.2241, 5.2295, 5.2267],\n",
      "        [5.2214, 5.2283, 4.9381],\n",
      "        [4.8794, 4.7601, 5.1313],\n",
      "        [5.1840, 5.2295, 5.0683],\n",
      "        [4.8859, 5.2295, 5.2075],\n",
      "        [5.1973, 5.0486, 5.0681],\n",
      "        [4.4367, 4.4212, 5.1623],\n",
      "        [5.1588, 5.0980, 5.1837],\n",
      "        [5.1110, 5.2295, 5.2295],\n",
      "        [4.6901, 4.8993, 5.1769],\n",
      "        [5.0000, 5.2295, 5.2189],\n",
      "        [5.2295, 5.2295, 5.0183],\n",
      "        [5.2295, 4.9930, 5.0597],\n",
      "        [5.2285, 4.6863, 4.8969],\n",
      "        [5.1228, 4.0413, 4.7938],\n",
      "        [5.2290, 4.3837, 5.1567],\n",
      "        [5.0569, 4.9930, 4.9996],\n",
      "        [5.2294, 5.2295, 5.0677],\n",
      "        [5.1800, 5.0742, 5.1893],\n",
      "        [5.2233, 5.0486, 5.0737],\n",
      "        [5.1192, 5.1893, 4.9858],\n",
      "        [5.2148, 5.2295, 5.1802],\n",
      "        [5.2144, 5.2295, 5.1569],\n",
      "        [5.1217, 5.2295, 4.8424],\n",
      "        [5.2253, 5.2295, 5.1862],\n",
      "        [5.2205, 5.0486, 5.0676]])\n"
     ]
    }
   ],
   "source": [
    "n, d = train_x.shape\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    kernel = model.covar_module\n",
    "\n",
    "    # Initialize the matrix K with zeros\n",
    "    K_per_feature = torch.zeros((n, d))\n",
    "\n",
    "    # Extracting a specific instance's features\n",
    "    instance_features = train_x[3].unsqueeze(0)  # Shape (1, d)\n",
    "\n",
    "    # Loop over each feature dimension\n",
    "    for i in range(d):\n",
    "        # Reshape the specific feature across all samples to match the input shape required by the kernel\n",
    "        feature_column = train_x[:, i].unsqueeze(1)  # Shape (n, 1)\n",
    "        instance_feature = instance_features[:, i].unsqueeze(1)  # Shape (1, 1)\n",
    "\n",
    "        K_per_feature[:, i] = kernel(instance_feature, feature_column).evaluate().squeeze()\n",
    "        print(K_per_feature[:, i])\n",
    "\n",
    "print(\"Matrix K (n*d):\")\n",
    "print(K_per_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brute force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the Shapley value: tensor([35.5546, 38.3088, 35.3558])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from itertools import chain, combinations\n",
    "\n",
    "# Assume train_x and K_per_feature are already defined and populated\n",
    "# Example input data\n",
    "\n",
    "\n",
    "n_samples, n_features = train_x.size()\n",
    "val = torch.zeros(n_features)\n",
    "\n",
    "# Calculate the extended K\n",
    "temp = range(n_features)\n",
    "feature_combinations = list(chain.from_iterable(combinations(temp, r) for r in range(n_features+1)))\n",
    "feature_combinations.remove(())\n",
    "\n",
    "extended_K = torch.zeros((n_samples, len(feature_combinations)))\n",
    "for i, comb in enumerate(feature_combinations):\n",
    "    product = torch.ones(n_samples)#.unsqueeze(0)\n",
    "\n",
    "    for j in comb:\n",
    "        product = product *K_per_feature[:, j]#.unsqueeze(0)\n",
    "    extended_K[:, i] = product\n",
    "\n",
    "# Create a loop for each feature to compute its Shapley value\n",
    "for j in range(n_features):\n",
    "    # Find the subsets where the j-th feature was used\n",
    "    indices_of_kj_columns = [idx for idx, combination in enumerate(feature_combinations) if j in combination]\n",
    "    indices_of_kj_columns = torch.tensor(indices_of_kj_columns) # Convert the list to a tensor\n",
    "\n",
    "    # Update the extended_K matrix to only include the columns with the indices found\n",
    "    updated_extended_K = torch.zeros(n_samples, len(indices_of_kj_columns))\n",
    "    for i, idx in enumerate(indices_of_kj_columns):\n",
    "        updated_extended_K[:, i] = extended_K[:, idx]\n",
    "\n",
    "    # Create a vector of weights for the corresponding columns\n",
    "    weights = torch.zeros((len(indices_of_kj_columns), 1))\n",
    "\n",
    "    # Set the weights as 1 / length of the combination\n",
    "    for i, idx in enumerate(indices_of_kj_columns):\n",
    "        weights[i] = 1 / len(feature_combinations[idx])\n",
    "        \n",
    "\n",
    "    # Compute omega\n",
    "    omega = torch.matmul(updated_extended_K, weights)\n",
    "    # omega = torch.zeros(n_samples,1)\n",
    "    # for i in range(len(indices_of_kj_columns)):\n",
    "    #     col = (updated_extended_K[:, i] * weights[i]).unsqueeze(1)\n",
    "        \n",
    "    \n",
    "    #     omega += col\n",
    "\n",
    "    # Compute the Shapley value for the j-th feature\n",
    "    val[j] = torch.matmul(omega.T, alpha_hat_eta)\n",
    "\n",
    "print('This is the Shapley value:', val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the Shapley value: tensor([35.5542, 38.3092, 35.3562])\n"
     ]
    }
   ],
   "source": [
    "# method by calculating it as a whole matrix\n",
    "# method xxxx\n",
    "n_samples, n_features = K_per_feature.shape\n",
    "val = torch.zeros(n_features)\n",
    "\n",
    "# Calculate the extended K\n",
    "temp = range(n_features)\n",
    "feature_combinations = list(chain.from_iterable(combinations(temp, r) for r in range(n_features+1)))\n",
    "feature_combinations.remove(())\n",
    "# print(feature_combinations)\n",
    "# Initialize the weights vector\n",
    "weights = torch.zeros(len(feature_combinations))\n",
    "\n",
    "# Calculate weights according to the rule\n",
    "for i in range(len(feature_combinations)):\n",
    "    weights[i] = 1 / len(feature_combinations[i])\n",
    "# print(weights)\n",
    "\n",
    "extended_K = torch.zeros((n_samples, len(feature_combinations)))\n",
    "for i, comb in enumerate(feature_combinations):\n",
    "    product = torch.ones(n_samples)#.unsqueeze(0)\n",
    "\n",
    "    for j in comb:\n",
    "        product = product *K_per_feature[:, j]#.unsqueeze(0)\n",
    "    extended_K[:, i] = product\n",
    "    # print(extended_K)\n",
    "\n",
    "\n",
    "#Create a loop for each feature to compute its Shapley value\n",
    "for j in range(n_features):\n",
    "    # Find the subsets where the j-th feature was used\n",
    "    indices_of_kj_columns = [idx for idx, combination in enumerate(feature_combinations) if j in combination]\n",
    "    indices_of_kj_columns = torch.tensor(indices_of_kj_columns) #\n",
    "    # print(indices_of_kj_columns)\n",
    "    vector = torch.zeros(len(feature_combinations))\n",
    "    vector[indices_of_kj_columns] = 1\n",
    "    # print(vector)\n",
    "\n",
    "    update_K = vector*extended_K \n",
    "    # print(update_K)\n",
    "\n",
    "    omega = torch.matmul(update_K, weights)\n",
    "    # print('omega',omega)\n",
    "    # Compute the Shapley value for the j-th feature\n",
    "    val[j] = torch.matmul(omega.T, alpha_hat_eta)\n",
    "\n",
    "print('This is the Shapley value:', val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is val: tensor([35.5542, 38.3091, 35.3563])\n"
     ]
    }
   ],
   "source": [
    "def Omega(X, i, q_additivity=None, feature_type='numerical'):\n",
    "    \n",
    "    \n",
    "    if q_additivity is None:\n",
    "        q_additivity = d\n",
    "    \n",
    "    # Reorder columns so that the i-th column is first\n",
    "    idx = torch.arange(d)\n",
    "    idx[i] = 0\n",
    "    idx[0] = i\n",
    "    X = X[:, idx]\n",
    "\n",
    "    # Initialize dp array\n",
    "    dp = torch.zeros((q_additivity, d, n))\n",
    "\n",
    "    # Initial sum of features across the dataset\n",
    "    sum_current = torch.zeros((n,))\n",
    "    \n",
    "    # Fill the first order dp (base case)\n",
    "    for j in range(d):\n",
    "        dp[0, j, :] = X[:, j]\n",
    "        sum_current += X[:, j]\n",
    "\n",
    "    # Fill the dp table for higher orders\n",
    "    for i in range(1, q_additivity):\n",
    "        temp_sum = torch.zeros((n,))\n",
    "        for j in range(d):\n",
    "            # Subtract the previous contribution of this feature when moving to the next order\n",
    "            sum_current -= dp[i - 1, j, :]\n",
    "            dp[i, j, :] = (i / (i + 1)) * (X[:,j]* sum_current)\n",
    "            temp_sum += dp[i, j, :]\n",
    "        \n",
    "        sum_current = temp_sum\n",
    "\n",
    "    # Sum up all contributions from the first dimension of each feature to get the final values\n",
    "    omega = torch.sum(dp[:, 0, :], axis=0)\n",
    "\n",
    "    return omega, dp\n",
    "\n",
    "\n",
    "val = torch.zeros(n_features)\n",
    "for i in range(n_features):\n",
    "    omega_dp, _ = Omega(K_per_feature, i, q_additivity=None, feature_type='numerical')\n",
    "    val[i] = torch.matmul(omega_dp, alpha_hat_eta)\n",
    "\n",
    "print('This is val:', val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
