{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "import tensorflow\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from linear_operator import to_dense\n",
    "from gpytorch.constraints import Positive\n",
    "from gpytorch.kernels import Kernel\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.],\n",
      "        [7., 8., 9.]])\n",
      "Shape of X_train: torch.Size([4, 3])\n",
      "Shape of y_train: torch.Size([4, 1])\n",
      "Shape of X_test: torch.Size([2, 3])\n",
      "Shape of y_test: torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "# Training data\n",
    "train_x = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                        [4.0, 5.0, 6.0],\n",
    "                        [7.0, 8.0, 9.0],\n",
    "                        [7.0, 8.0, 9.0]])\n",
    "y_train = torch.tensor([[2.0],\n",
    "                        [3.0],\n",
    "                        [4.0],\n",
    "                        [5.0]])\n",
    "\n",
    "# Test data\n",
    "test_x = torch.tensor([[4.0, 1.0, 2.0],\n",
    "                       [3.0, 4.0, 5.0]])\n",
    "y_test = torch.tensor([[1.0],\n",
    "                       [2.0]])\n",
    "\n",
    "# Display shapes of the tensors\n",
    "print(train_x)\n",
    "print(\"Shape of X_train:\", train_x.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of X_test:\", test_x.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Additive k(xi, xj) = -1 + ‚àè (1+k(x,y)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: likelihood.noise_covar.raw_noise           value = tensor([-0.7339])\n",
      "Parameter name: covar_module.raw_outputscale               value = 0.5714855194091797\n",
      "Parameter name: covar_module.base_kernel.raw_lengthscale   value = tensor([[5.3620]])\n",
      "[1.9297421 2.6502414]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**Printing all model constraints...**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint name: likelihood.noise_covar.raw_noise_constraint             constraint = GreaterThan(1.000E-04)\n",
      "Constraint name: covar_module.raw_outputscale_constraint                 constraint = Positive()\n",
      "Constraint name: covar_module.base_kernel.raw_lengthscale_constraint     constraint = Positive()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class FullyAdditive(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, base_kernel: Kernel, num_dims = int, active_dims: Optional[Tuple[int, ...]] = None,  **kwargs):\n",
    "        super(FullyAdditive, self).__init__(active_dims=active_dims, **kwargs)\n",
    "        self.base_kernel = base_kernel\n",
    "        self.num_dims = num_dims\n",
    "        outputscale_constraint = gpytorch.constraints.Positive()\n",
    "        #here register one outputscale parameter\n",
    "        self.register_parameter(name=\"raw_outputscale\", \n",
    "            parameter=torch.nn.Parameter(torch.tensor(1.0)))\n",
    "        outputscale_constraint = gpytorch.constraints.Positive()\n",
    "        self.register_constraint(\"raw_outputscale\", outputscale_constraint)\n",
    "\n",
    "    @property\n",
    "    def outputscale(self):\n",
    "        return self.raw_outputscale_constraint.transform(self.raw_outputscale)\n",
    "\n",
    "    @outputscale.setter\n",
    "    def outputscale(self, value):\n",
    "        self._set_outputscale(value)\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "            # Initialize product terms correctly for each pair of inputs\n",
    "            prod_terms = 1.0  # Start with scalar 1 for multiplication\n",
    "            #calculate the kernel for each dimension\n",
    "            for d in range(self.num_dims):\n",
    "                x1_d = x1[:, d:d+1]  # Isolate the d-th dimension\n",
    "                x2_d = x2[:, d:d+1]\n",
    "                # print(x1_d)\n",
    "                k_d = self.base_kernel(x1_d, x2_d, **params).evaluate()\n",
    "                # print(k_d)  # Use evaluate() to get the kernel matrix\n",
    "                prod_terms *= (1 +self.outputscale* k_d) #calculate the multiplication of a sequence of terms\n",
    "            # print(prod_terms)\n",
    "\n",
    "            K = -1 + prod_terms \n",
    "            # print('k shape',K.shape)\n",
    "\n",
    "            return  K\n",
    "    \n",
    "\n",
    "class MyGP(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        dimensionality = train_x.size(-1)  # Assuming last dimension is the dimensionality \n",
    "\n",
    "        #base_kernel = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()) #when using the scaleKernel it gives the sigma f for the base kernel, using only the RBF - it has no ouputscale\n",
    "        base_kernel = gpytorch.kernels.RBFKernel() \n",
    "        num_dims = dimensionality\n",
    "        self.covar_module = FullyAdditive(base_kernel, num_dims)\n",
    "\n",
    "    def forward(self, x): \n",
    "        mean = self.mean_module(x)\n",
    "        covar = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean, covar)\n",
    "\n",
    "\n",
    "# Create the GP model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "model = MyGP(train_x, y_train.squeeze(-1), likelihood)\n",
    "# Set up optimizer and marginal log likelihood\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "# Training loop\n",
    "training_iter = 50\n",
    "for i in range(training_iter):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    #print(y_train.squeeze().size())\n",
    "    loss = -mll(output, y_train.squeeze(-1))\n",
    "    loss = loss.mean() \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# Viewing model parameters after training\n",
    "for param_name, param in model.named_parameters():\n",
    "    print(f'Parameter name: {param_name:42} value = {param.data}')\n",
    "\n",
    "\n",
    "#evaluating there is a problem when the test_y and test_x have float numbers\n",
    "with torch.no_grad():\n",
    "    model.eval()  # Set the model to evaluation mode (mode is for computing predictions through the model posterior.)\n",
    "    output = model(test_x)  # Make predictions on new data \n",
    "    \n",
    "\n",
    "# Step 4: Postprocess Predictions (if needed)\n",
    "predicted_means = output.mean.numpy() \n",
    "print(predicted_means)\n",
    "printmd('\\n\\n**Printing all model constraints...**\\n')\n",
    "for constraint_name, constraint in model.named_constraints():\n",
    "    print(f'Constraint name: {constraint_name:55} constraint = {constraint}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: likelihood.noise_covar.raw_noise           value = tensor([-0.7604])\n",
      "Parameter name: base_kernel.raw_lengthscale                value = tensor([[5.4945]])\n",
      "Parameter name: covar_module.raw_outputscale               value = tensor([[ 1.5825, -0.3505, -1.1057]])\n",
      "[2.0288777 2.687921 ]\n",
      "Constraint name: likelihood.noise_covar.raw_noise_constraint             constraint = GreaterThan(1.000E-04)\n",
      "Constraint name: base_kernel.raw_lengthscale_constraint                  constraint = Positive()\n",
      "Constraint name: covar_module.raw_outputscale_constraint                 constraint = Positive()\n"
     ]
    }
   ],
   "source": [
    "class DPkernel(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, base_kernel, num_dims, q_additivity, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.base_kernel = base_kernel\n",
    "        self.num_dims = num_dims\n",
    "        self.q_additivity = q_additivity\n",
    "        self.register_parameter(\n",
    "            name=\"raw_outputscale\", \n",
    "            parameter=torch.nn.Parameter(torch.zeros(1, self.q_additivity))\n",
    "        )\n",
    "        self.outputscale_constraint = gpytorch.constraints.Positive()\n",
    "        self.register_constraint(\"raw_outputscale\", self.outputscale_constraint)\n",
    "\n",
    "    @property\n",
    "    def outputscale(self):\n",
    "        return self.outputscale_constraint.transform(self.raw_outputscale).squeeze()\n",
    "\n",
    "    @outputscale.setter\n",
    "    def outputscale(self, value):\n",
    "        if not torch.is_tensor(value):\n",
    "            value = torch.tensor(value, device=self.raw_outputscale.device)\n",
    "        self.initialize(raw_outputscale=self.outputscale_constraint.inverse_transform(value))\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "    # Determine sizes based on input matrices\n",
    "        x1_size = x1.size(0)\n",
    "        x2_size = x2.size(0)\n",
    "        \n",
    "        # Initialize matrices based on input sizes\n",
    "        result = torch.zeros(x1_size, x2_size, device=x1.device) #initialize the result matrix\n",
    "        sum_order_b = torch.zeros(x1_size, x2_size, device=x1.device) # initialize the matrix for the matrix for a single order\n",
    "        kernels =[] # list were the z1, z2,... would be stored\n",
    "\n",
    "        # print(f\"Initial x1 shape: {x1.shape}, x2 shape: {x2.shape}\")\n",
    "        \n",
    "        #calculations for first order\n",
    "        #calcualte the kernels for each dimentions\n",
    "        for d in range(self.num_dims):\n",
    "            x1_d = x1[:, d:d+1]\n",
    "            x2_d = x2[:, d:d+1]\n",
    "            k_d = self.base_kernel(x1_d, x2_d).evaluate()\n",
    "            kernels.append(k_d) #save them in order in the kernels list\n",
    "            # print(f\"Kernel k_d at dim {d} shape: {k_d.shape}, sum_order_b shape: {sum_order_b.shape}\")\n",
    "\n",
    "            sum_order_b += k_d # add each one dimension kernels to one matrix for first order\n",
    "\n",
    "        result += sum_order_b * self.outputscale[0] #add the first order kernel miltiplied by first outputscale\n",
    "\n",
    "        #calculations for higher dimensions\n",
    "        for i in range(1, self.q_additivity):\n",
    "            temp_sum = torch.zeros(x1_size, x2_size, device=x1.device)\n",
    "            new_kernels = []\n",
    "            for j in range(self.num_dims):\n",
    "                sum_order_b = sum_order_b - kernels[j]\n",
    "                k_d = kernels[j] * sum_order_b\n",
    "                new_kernels.append(k_d)\n",
    "                temp_sum += k_d\n",
    "\n",
    "            sum_order_b = temp_sum\n",
    "            kernels = new_kernels\n",
    "            result += sum_order_b * self.outputscale[i]\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "# Example usage in a GP model\n",
    "class MyGP(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(MyGP, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.base_kernel = gpytorch.kernels.RBFKernel()\n",
    "        self.covar_module = DPkernel(base_kernel=self.base_kernel, num_dims=train_x.size(-1), q_additivity=train_x.size(-1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x, x)  # Make sure to pass x twice\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Create the GP model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "model = MyGP(train_x, y_train.squeeze(-1), likelihood)\n",
    "# Set up optimizer and marginal log likelihood\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "training_iter = 50\n",
    "for i in range(training_iter):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    # print(output)\n",
    "    loss = -mll(output, y_train.squeeze())\n",
    "    # print(loss)\n",
    "    loss = loss.mean() \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "# Viewing model parameters after training\n",
    "for param_name, param in model.named_parameters():\n",
    "    print(f'Parameter name: {param_name:42} value = {param.data}')\n",
    "\n",
    "\n",
    "#evaluating there is a problem when the test_y and test_x have float numbers\n",
    "with torch.no_grad():\n",
    "    model.eval()  # Set the model to evaluation mode (mode is for computing predictions through the model posterior.)\n",
    "    output = model(test_x)  # Make predictions on new data \n",
    "\n",
    "# Step 4: Postprocess Predictions (if needed)\n",
    "predicted_means = output.mean.numpy() \n",
    "print(predicted_means)\n",
    "for constraint_name, constraint in model.named_constraints():\n",
    "    print(f'Constraint name: {constraint_name:55} constraint = {constraint}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in both i didint have to cancel the sigma f for RBF kernel function as it doesnt have a outputscale. Adding the output scale is using the ScaleKernel()\n",
    "\n",
    "# just don't understand why do the optimized parameters are sometimes negative if the constraints are positive\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
