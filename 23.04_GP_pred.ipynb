{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score,train_test_split\n",
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "import tensorflow\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from linear_operator import to_dense\n",
    "from gpytorch.constraints import Positive\n",
    "from gpytorch.kernels import Kernel\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from gpytorch.settings import cholesky_jitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 414 entries, 0 to 413\n",
      "Data columns (total 7 columns):\n",
      " #   Column                                  Non-Null Count  Dtype  \n",
      "---  ------                                  --------------  -----  \n",
      " 0   X1 transaction date                     414 non-null    float64\n",
      " 1   X2 house age                            414 non-null    float64\n",
      " 2   X3 distance to the nearest MRT station  414 non-null    float64\n",
      " 3   X4 number of convenience stores         414 non-null    int64  \n",
      " 4   X5 latitude                             414 non-null    float64\n",
      " 5   X6 longitude                            414 non-null    float64\n",
      " 6   Y house price of unit area              414 non-null    float64\n",
      "dtypes: float64(6), int64(1)\n",
      "memory usage: 22.8 KB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('Real estate.csv') #https://www.kaggle.com/datasets/quantbruce/real-estate-price-prediction?resource=download\n",
    "data = data.drop(['No'], axis=1)\n",
    "data.info()\n",
    "data = data.dropna()\n",
    "# Display the first few rows of the DataFrame\n",
    "# print(data.head())\n",
    "x = data.drop(columns=['Y house price of unit area'])\n",
    "y = data['Y house price of unit area']\n",
    "train_x,test_x,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "train_x_scaled = scaler.fit_transform(train_x) # computes the mean and standard deviation for each feature in train_x and scales train_x using these statistics\n",
    "test_x_scaled = scaler.transform(test_x)\n",
    "\n",
    "\n",
    "# Convert the pandas DataFrame/Series to PyTorch tensors\n",
    "train_x = torch.tensor(train_x_scaled, dtype=torch.float32)\n",
    "test_x = torch.tensor(test_x_scaled, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# print(train_x)\n",
    "# print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton Girard Formulae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the Newton Girard Formulae kernel function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewtonGirardAdditiveKernel(Kernel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_kernel: Kernel,\n",
    "        num_dims: int,\n",
    "        max_degree: Optional[int] = None,\n",
    "        active_dims: Optional[Tuple[int, ...]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Create an Additive Kernel a la https://arxiv.org/abs/1112.4394 using Newton-Girard Formulae\n",
    "\n",
    "        :param base_kernel: a base 1-dimensional kernel. NOTE: put ard_num_dims=d in the base kernel...\n",
    "        :param max_degree: the maximum numbers of kernel degrees to compute\n",
    "        :param active_dims:\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "        super(NewtonGirardAdditiveKernel, self).__init__(active_dims=active_dims, **kwargs)\n",
    "\n",
    "        self.base_kernel = base_kernel\n",
    "        self.num_dims = num_dims\n",
    "        if max_degree is None:\n",
    "            self.max_degree = self.num_dims\n",
    "        elif max_degree > self.num_dims:  # force cap on max_degree (silently)\n",
    "            self.max_degree = self.num_dims\n",
    "        else:\n",
    "            self.max_degree = max_degree\n",
    "\n",
    "        self.register_parameter(\n",
    "            name=\"raw_outputscale\", parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, self.max_degree))\n",
    "        )\n",
    "        outputscale_constraint = Positive()\n",
    "        self.register_constraint(\"raw_outputscale\", outputscale_constraint)\n",
    "        self.outputscale_constraint = outputscale_constraint\n",
    "        self.outputscale = [1 / self.max_degree for _ in range(self.max_degree)]\n",
    "\n",
    "    @property\n",
    "    def outputscale(self):\n",
    "        return self.raw_outputscale_constraint.transform(self.raw_outputscale)\n",
    "\n",
    "    @outputscale.setter\n",
    "    def outputscale(self, value):\n",
    "        self._set_outputscale(value)\n",
    "\n",
    "    def _set_outputscale(self, value):\n",
    "        if not torch.is_tensor(value):\n",
    "            value = torch.as_tensor(value).to(self.raw_outputscale)\n",
    "\n",
    "        self.initialize(raw_outputscale=self.outputscale_constraint.inverse_transform(value))\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):\n",
    "        \"\"\"Forward proceeds by Newton-Girard formulae\"\"\"\n",
    "        if last_dim_is_batch:\n",
    "            raise RuntimeError(\"NewtonGirardAdditiveKernel does not accept the last_dim_is_batch argument.\")\n",
    "\n",
    "        # NOTE: comments about shape are only correct for the single-batch cases.\n",
    "        # kern_values is just the order-1 terms\n",
    "        # kern_values = D x n x n unless diag=True\n",
    "        kern_values = to_dense(self.base_kernel(x1, x2, diag=diag, last_dim_is_batch=True, **params))\n",
    "\n",
    "        # last dim is batch, which gets moved up to pos. \n",
    "\n",
    "\n",
    "        kernel_dim = -3 if not diag else -2\n",
    "\n",
    "        shape = [1 for _ in range(len(kern_values.shape) + 1)]\n",
    "      \n",
    "        shape[kernel_dim - 1] = -1\n",
    "        kvals = torch.arange(1, self.max_degree + 1, device=kern_values.device).reshape(*shape)\n",
    "        # kvals = R x 1 x 1 x 1 (these are indexes only)\n",
    "\n",
    "        # e_n = torch.ones(self.max_degree+1, *kern_values.shape[1:], device=kern_values.device)  # includes 0\n",
    "        # e_n: elementary symmetric polynomial of degree n (e.g. z1 z2 + z1 z3 + z2 z3)\n",
    "        # e_n is R x n x n, and the array is properly 0 indexed.\n",
    "        shape = [d_ for d_ in kern_values.shape]\n",
    "        shape[kernel_dim] = self.max_degree + 1\n",
    "        e_n = torch.empty(*shape, device=kern_values.device)\n",
    "        if kernel_dim == -3:\n",
    "            e_n[..., 0, :, :] = 1.0\n",
    "        else:\n",
    "            e_n[..., 0, :] = 1.0\n",
    "\n",
    "        # power sums s_k (e.g. sum_i^num_dims z_i^k\n",
    "        # s_k is R x n x n\n",
    "        s_k = kern_values.unsqueeze(kernel_dim - 1).pow(kvals).sum(dim=kernel_dim)\n",
    "\n",
    "        # just the constant -1\n",
    "        m1 = torch.tensor([-1], dtype=torch.float, device=kern_values.device)\n",
    "\n",
    "        shape = [1 for _ in range(len(kern_values.shape))]\n",
    "        shape[kernel_dim] = -1\n",
    "        for deg in range(1, self.max_degree + 1):  # deg goes from 1 to R (it's 1-indexed!)\n",
    "            # we avg over k [1, ..., deg] (-1)^(k-1)e_{deg-k} s_{k}\n",
    "\n",
    "            ks = torch.arange(1, deg + 1, device=kern_values.device, dtype=torch.float).reshape(*shape)  # use for pow\n",
    "            kslong = torch.arange(1, deg + 1, device=kern_values.device, dtype=torch.long)  # use for indexing\n",
    "\n",
    "            # note that s_k is 0-indexed, so we must subtract 1 from kslong\n",
    "            sum_ = (\n",
    "                m1.pow(ks - 1) * e_n.index_select(kernel_dim, deg - kslong) * s_k.index_select(kernel_dim, kslong - 1)\n",
    "            ).sum(dim=kernel_dim) / deg\n",
    "            if kernel_dim == -3:\n",
    "                e_n[..., deg, :, :] = sum_\n",
    "            else:\n",
    "                e_n[..., deg, :] = sum_\n",
    "\n",
    "        if kernel_dim == -3:\n",
    "            return (self.outputscale.unsqueeze(-1).unsqueeze(-1) * e_n.narrow(kernel_dim, 1, self.max_degree)).sum(\n",
    "                dim=kernel_dim\n",
    "            )\n",
    "        else:\n",
    "            return (self.outputscale.unsqueeze(-1) * e_n.narrow(kernel_dim, 1, self.max_degree)).sum(dim=kernel_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the code with GPytorch predictions (newton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: likelihood.noise_covar.raw_noise           value = tensor([5.1058])\n",
      "Parameter name: covar_module.raw_outputscale               value = tensor([13.0054,  3.5289,  0.9505,  1.1713,  1.5054,  1.8005])\n",
      "Parameter name: covar_module.base_kernel.raw_lengthscale   value = tensor([[-1.3867]])\n",
      "Constraint name: likelihood.noise_covar.raw_noise_constraint             constraint = GreaterThan(1.000E-04)\n",
      "Constraint name: covar_module.raw_outputscale_constraint                 constraint = Positive()\n",
      "Constraint name: covar_module.base_kernel.raw_lengthscale_constraint     constraint = Positive()\n",
      "Predicted Means:\n",
      "[48.73434   38.872406  44.027412  36.111267  28.714663  48.152573\n",
      " 45.783546  45.607086  14.170515  58.75998   24.513943  32.307228\n",
      " 29.93573   14.751556  32.472305  24.473934  38.846596  50.589516\n",
      " 23.799442  39.33092    9.840057  24.759457  49.933304  45.647102\n",
      " 14.8702545 33.56215   16.175327  46.18608   46.306427  40.907146\n",
      " 18.738731  30.415497  35.968544  26.347605  47.361305  35.00298\n",
      " 53.51751   15.5136385 44.381107  42.879486  46.851807  43.64338\n",
      " 42.52279   35.689972  38.097992  46.42      34.678463  17.268608\n",
      " 47.59129   42.422867  46.402996  54.4327    37.254562  34.482605\n",
      " 37.78283   16.102018  35.534157  26.287178  27.174896  49.987793\n",
      " 25.582176  24.892334  16.102028  12.694033  19.620293  25.73413\n",
      " 22.308926  37.32615   34.048744  28.04411   42.764694  57.69347\n",
      " 52.69307   51.47045   32.808983  35.648453  38.860374  35.09509\n",
      " 35.80462   23.724384  38.746193  46.628906  26.549046 ]\n",
      "Predicted Standard Deviations:\n",
      "[ 3.1030817  6.1581516  3.1494722  6.6598806  7.585428   8.822975\n",
      "  7.5994415  6.5477915  3.603164   3.119263  10.248063   2.7403197\n",
      " 10.87087    7.643281   9.666404   3.0204635  7.5526967  8.744165\n",
      "  7.508621   7.8013315 12.531885   6.9679666  5.8755393  7.0072346\n",
      "  8.419867   3.1169212  6.8640943  7.7320814  8.940851  10.195777\n",
      "  8.137935   8.764303   6.9529214  3.153245   7.4727116  7.8927193\n",
      "  6.9895177  3.0659714  9.505968   8.88274   10.594774   8.088939\n",
      "  8.131349   9.808339   6.8431654  8.1716385  8.063251  11.22423\n",
      "  3.151767   7.9159465  4.217359   3.120045   7.9458017  8.025748\n",
      "  9.766333   9.103762   9.530949  11.37756    7.428342   7.017882\n",
      "  8.728416  10.32226    9.103762   8.7972    10.733064   8.606225\n",
      "  8.2507715  8.641477   8.667908   3.1780775  7.576765   5.0412908\n",
      "  7.7908893  6.6492753  9.34751    8.000506   7.5749636 10.417494\n",
      " 10.161886  10.929887   8.66672    6.6126504  8.253236 ]\n"
     ]
    }
   ],
   "source": [
    "class MyGP(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        dimensionality = train_x.size(-1)  # Assuming last dimension is the dimensionality\n",
    "        base_kernel = gpytorch.kernels.RBFKernel()  # Or any other base kernel you want to use\n",
    "        num_dims = dimensionality\n",
    "        self.covar_module = NewtonGirardAdditiveKernel(base_kernel, num_dims)\n",
    "\n",
    "    def forward(self, x): \n",
    "        mean = self.mean_module(x)\n",
    "        covar = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean, covar)\n",
    "\n",
    "\n",
    "# Create the GP model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "model = MyGP(train_x, y_train.squeeze(-1), likelihood)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    untrained_pred_dist = likelihood(model(test_x))\n",
    "    predictive_mean = untrained_pred_dist.mean\n",
    "    lower, upper = untrained_pred_dist.confidence_region()\n",
    "# Set up optimizer and marginal log likelihood\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "# print(output.mean.shape)  # Check the shape of the mean\n",
    "# print(output.variance.shape)  # Check the shape of the variance\n",
    "# print(y_train.squeeze(-1).shape)\n",
    "\n",
    "# print(y_train.size())\n",
    "model.train()\n",
    "likelihood.train()\n",
    "# Training loop\n",
    "training_iter = 1000\n",
    "for i in range(training_iter):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    # print(output)\n",
    "    loss = -mll(output, y_train)\n",
    "    loss = loss.mean() \n",
    "    loss.backward()\n",
    "    # print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "    #     i + 1, training_iter, loss.item(),\n",
    "    #     model.covar_module.base_kernel.lengthscale.item(),\n",
    "    #     model.likelihood.noise.item()\n",
    "    # ))\n",
    "    optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    trained_pred_dist = likelihood(model(test_x))\n",
    "    predictive_mean = trained_pred_dist.mean\n",
    "    lower, upper = trained_pred_dist.confidence_region()\n",
    "# Viewing model parameters after training\n",
    "for param_name, param in model.named_parameters():\n",
    "    print(f'Parameter name: {param_name:42} value = {param.data}')\n",
    "\n",
    "\n",
    "#evaluating there is a problem when the test_y and test_x have float numbers\n",
    "with torch.no_grad():\n",
    "    model.eval()  # Set the model to evaluation mode (mode is for computing predictions through the model posterior.)\n",
    "    likelihood.eval()\n",
    "    output = likelihood(model(test_x))  # Make predictions on new data \n",
    "    \n",
    "\n",
    "\n",
    "for constraint_name, constraint in model.named_constraints():\n",
    "    print(f'Constraint name: {constraint_name:55} constraint = {constraint}')\n",
    "\n",
    "\n",
    "# Extracting means and standard deviations\n",
    "predicted_means = output.mean.numpy() \n",
    "predicted_stddevs = output.stddev.numpy()  # Extract standard deviations\n",
    "\n",
    "print(\"Predicted Means:\")\n",
    "print(predicted_means)\n",
    "\n",
    "print(\"Predicted Standard Deviations:\")\n",
    "print(predicted_stddevs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error 4.239747\n",
      "Mean Square Error 39.729267\n",
      "Root Mean Square Error 6.303116\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error,mean_squared_error, mean_absolute_percentage_error \n",
    "mae = mean_absolute_error(y_true=y_test, \n",
    "                          y_pred=predicted_means) \n",
    "print(\"Mean Absolute Error\", mae) \n",
    "\n",
    "mse = mean_squared_error(y_true=y_test, \n",
    "                          y_pred=predicted_means) \n",
    "print(\"Mean Square Error\", mse)\n",
    "\n",
    "\n",
    "rmse = mean_squared_error(y_true=y_test, \n",
    "                          y_pred=predicted_means,\n",
    "                          squared=False) \n",
    "print(\"Root Mean Square Error\", rmse) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the math formula (newton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: likelihood.noise_covar.raw_noise           value = tensor([5.1058])\n",
      "Parameter name: covar_module.raw_outputscale               value = tensor([13.0054,  3.5289,  0.9505,  1.1713,  1.5054,  1.8005])\n",
      "Parameter name: covar_module.base_kernel.raw_lengthscale   value = tensor([[-1.3867]])\n",
      "Predicted Means: tensor([48.7340, 38.8721, 44.0266, 36.1102, 28.7152, 48.1524, 45.7826, 45.6059,\n",
      "        14.1703, 58.7590, 24.5139, 32.3072, 29.9358, 14.7514, 32.4720, 24.4727,\n",
      "        38.8458, 50.5899, 23.7994, 39.3306,  9.8399, 24.7585, 49.9329, 45.6467,\n",
      "        14.8698, 33.5613, 16.1754, 46.1854, 46.3066, 40.9064, 18.7390, 30.4153,\n",
      "        35.9683, 26.3478, 47.3605, 35.0029, 53.5174, 15.5136, 44.3811, 42.8791,\n",
      "        46.8518, 43.6434, 42.5226, 35.6895, 38.0978, 46.4193, 34.6776, 17.2684,\n",
      "        47.5913, 42.4224, 46.4023, 54.4327, 37.2544, 34.4825, 37.7829, 16.1016,\n",
      "        35.5342, 26.2869, 27.1752, 49.9869, 25.5816, 24.8923, 16.1016, 12.6942,\n",
      "        19.6203, 25.7337, 22.3084, 37.3252, 34.0483, 28.0431, 42.7641, 57.6942,\n",
      "        52.6935, 51.4705, 32.8083, 35.6479, 38.8603, 35.0948, 35.8050, 23.7241,\n",
      "        38.7459, 46.6278, 26.5487])\n",
      "Predicted Standard Deviations: tensor([ 2.1241,  5.7279,  2.1914,  6.2642,  7.2405,  8.5283,  7.2552,  6.1449,\n",
      "         2.8045,  2.1482,  9.9955,  1.5474, 10.6331,  7.3011,  9.3982,  2.0017,\n",
      "         7.2062,  8.4467,  7.1599,  7.4664, 12.3262,  6.5907,  5.4228,  6.6323,\n",
      "         8.1104,  2.1446,  6.4810,  7.3941,  8.6501,  9.9419,  7.8174,  8.4675,\n",
      "         6.5748,  2.1967,  7.1223,  7.5618,  6.6133,  2.0694,  9.2331,  8.5901,\n",
      "        10.3507,  7.7664,  7.8106,  9.5441,  6.4587,  7.8525,  7.7397, 10.9941,\n",
      "         2.1945,  7.5860,  3.5595,  2.1489,  7.6172,  7.7006,  9.5009,  8.8184,\n",
      "         9.2588, 11.1506,  7.0757,  6.6436,  8.4304, 10.0715,  8.8184,  8.5015,\n",
      "        10.4921,  8.3038,  7.9348,  8.3404,  8.3677,  2.2323,  7.2314,  4.5054,\n",
      "         7.4554,  6.2528,  9.0699,  7.6742,  7.2295, 10.1691,  9.9071, 10.6934,\n",
      "         8.3665,  6.2139,  7.9374])\n",
      "likelihood tensor([5.1119], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Viewing model parameters after training\n",
    "for param_name, param in model.named_parameters():\n",
    "    print(f'Parameter name: {param_name:42} value = {param.data}')\n",
    "\n",
    "# Ensure model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "likelihood.eval()\n",
    "with torch.no_grad():\n",
    "    #this is the training kernel matrix (K(X,X))\n",
    "    t_k_matrix = model.covar_module(train_x).evaluate()\n",
    "    #this is the noise matrix\n",
    "    n_matrix = torch.eye(t_k_matrix.size(-1)).to(t_k_matrix.device) * likelihood.noise_covar.raw_noise\n",
    "    # This is (K + sigma^2_n * I)^(-1)\n",
    "    K_inv = torch.inverse(t_k_matrix + n_matrix)\n",
    "    # alpha it the K_inv * y\n",
    "    alpha = torch.matmul(K_inv, model.train_targets.unsqueeze(-1))\n",
    "    # This is kernel matrix between the test points and training points, K(x*,X)\n",
    "    K_star = model.covar_module(test_x, train_x).evaluate()\n",
    "    #This is th ekernel matrix between all pairs of the test points (K(x*,x*))\n",
    "    K_star_star = model.covar_module(test_x).evaluate()\n",
    "\n",
    "    # Predictive mean calculation (mu_*)\n",
    "    pred_mean = torch.matmul(K_star, alpha)\n",
    "    # Predictive variance calculation (sigma^2_*)\n",
    "    pred_covar = K_star_star - torch.matmul(K_star, torch.matmul(K_inv, K_star.transpose(-1, -2)))\n",
    "\n",
    "    # Extract standard deviations from the covariance matrix\n",
    "    pred_stddev = torch.sqrt(torch.diag(pred_covar))\n",
    "\n",
    "# You can now print or return the predicted means and standard deviations\n",
    "print(\"Predicted Means:\", pred_mean.squeeze(-1))\n",
    "print(\"Predicted Standard Deviations:\", pred_stddev)\n",
    "print('likelihood', likelihood.noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error 4.239864\n",
      "Mean Square Error 39.73109\n",
      "Root Mean Square Error 6.3032603\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "mae = mean_absolute_error(y_true=y_test, \n",
    "                          y_pred=pred_mean) \n",
    "print(\"Mean Absolute Error\", mae) \n",
    "\n",
    "mse = mean_squared_error(y_true=y_test, \n",
    "                          y_pred=pred_mean) \n",
    "print(\"Mean Square Error\", mse)\n",
    "\n",
    "\n",
    "rmse = mean_squared_error(y_true=y_test, \n",
    "                          y_pred=pred_mean,\n",
    "                          squared=False) \n",
    "print(\"Root Mean Square Error\", rmse) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained model MSE: 45.04, \n",
      "Trained model MSE: 39.73\n",
      "Untrained model MAE: 4.65, \n",
      "Trained model MAE: 4.24\n"
     ]
    }
   ],
   "source": [
    "init_mse = gpytorch.metrics.mean_squared_error(untrained_pred_dist, y_test, squared=True)\n",
    "final_mse = gpytorch.metrics.mean_squared_error(trained_pred_dist, y_test, squared=True)\n",
    "\n",
    "print(f'Untrained model MSE: {init_mse:.2f}, \\nTrained model MSE: {final_mse:.2f}')\n",
    "\n",
    "init_mae = gpytorch.metrics.mean_absolute_error(untrained_pred_dist, y_test)\n",
    "final_mae = gpytorch.metrics.mean_absolute_error(trained_pred_dist, y_test)\n",
    "\n",
    "print(f'Untrained model MAE: {init_mae:.2f}, \\nTrained model MAE: {final_mae:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DP approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DP kernel function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPkernel(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, base_kernel, num_dims, q_additivity, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.base_kernel = base_kernel\n",
    "        self.num_dims = num_dims\n",
    "        self.q_additivity = q_additivity\n",
    "        self.register_parameter(\n",
    "            name=\"raw_outputscale\", \n",
    "            parameter=torch.nn.Parameter(torch.zeros(1, self.q_additivity))\n",
    "        )\n",
    "        self.outputscale_constraint = gpytorch.constraints.Positive()\n",
    "        self.register_constraint(\"raw_outputscale\", self.outputscale_constraint)\n",
    "\n",
    "    @property\n",
    "    def outputscale(self):\n",
    "        return self.outputscale_constraint.transform(self.raw_outputscale).squeeze()\n",
    "\n",
    "    @outputscale.setter\n",
    "    def outputscale(self, value):\n",
    "        if not torch.is_tensor(value):\n",
    "            value = torch.tensor(value, device=self.raw_outputscale.device)\n",
    "        self.initialize(raw_outputscale=self.outputscale_constraint.inverse_transform(value))\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "    # Determine sizes based on input matrices\n",
    "        x1_size = x1.size(0)\n",
    "        x2_size = x2.size(0)\n",
    "        \n",
    "        # Initialize matrices based on input sizes\n",
    "        result = torch.zeros(x1_size, x2_size, device=x1.device) #initialize the result matrix\n",
    "        sum_order_b = torch.zeros(x1_size, x2_size, device=x1.device) # initialize the matrix for the matrix for a single order\n",
    "        kernels =[] # list were the z1, z2,... would be stored\n",
    "\n",
    "        # print(f\"Initial x1 shape: {x1.shape}, x2 shape: {x2.shape}\")\n",
    "        \n",
    "        #calculations for first order\n",
    "        #calcualte the kernels for each dimentions\n",
    "        for d in range(self.num_dims):\n",
    "            x1_d = x1[:, d:d+1]\n",
    "            x2_d = x2[:, d:d+1]\n",
    "            k_d = self.base_kernel(x1_d, x2_d).evaluate()\n",
    "            kernels.append(k_d) #save them in order in the kernels list\n",
    "            # print(f\"Kernel k_d at dim {d} shape: {k_d.shape}, sum_order_b shape: {sum_order_b.shape}\")\n",
    "\n",
    "            sum_order_b += k_d # add each one dimension kernels to one matrix for first order\n",
    "\n",
    "        result += sum_order_b * self.outputscale[0] #add the first order kernel miltiplied by first outputscale\n",
    "\n",
    "        # Compute higher order interactions\n",
    "        for i in range(1, self.q_additivity):\n",
    "            temp_sum = torch.zeros(x1_size, x2_size, device=x1.device)\n",
    "            new_kernels = []\n",
    "            for j in range(self.num_dims):\n",
    "                for k in range(j + 1, self.num_dims):\n",
    "                    new_kernel = kernels[j] * kernels[k]\n",
    "                    new_kernels.append(new_kernel)\n",
    "                    temp_sum += new_kernel\n",
    "\n",
    "            kernels = new_kernels  # update kernels list with new order interactions\n",
    "            result += temp_sum * self.outputscale[i]\n",
    "\n",
    "        return result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the code with GPytorch predictions DP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: likelihood.noise_covar.raw_noise           value = tensor([10.7944])\n",
      "Parameter name: base_kernel.raw_lengthscale                value = tensor([[-0.2703]])\n",
      "Parameter name: covar_module.raw_outputscale               value = tensor([[18.9805,  0.4021, -4.8879, -4.8592, -3.6123,  3.6725]])\n",
      "Constraint name: likelihood.noise_covar.raw_noise_constraint             constraint = GreaterThan(1.000E-04)\n",
      "Constraint name: base_kernel.raw_lengthscale_constraint                  constraint = Positive()\n",
      "Constraint name: covar_module.raw_outputscale_constraint                 constraint = Positive()\n",
      "Predicted Means:\n",
      "[48.30478   36.26541   44.5175    42.56601   22.602028  46.21573\n",
      " 45.832596  46.602676  14.5451355 58.374115  24.146881  32.236923\n",
      " 36.057983  17.502106  33.503197  25.148178  44.687363  47.98326\n",
      " 24.895126  39.810844   9.809639  26.884705  53.238693  44.00853\n",
      " 16.462639  35.190857  16.391552  53.190887  40.439262  41.478546\n",
      " 17.46334   33.521492  37.44832   25.873825  45.11673   38.119278\n",
      " 55.832047  15.73925   44.47795   45.4264    49.46846   39.781067\n",
      " 43.641083  37.12628   36.474884  48.997604  36.740776  19.128983\n",
      " 48.1855    41.285637  48.74028   54.6839    39.354446  40.14209\n",
      " 37.167587  19.250008  35.62796   29.312897  22.467018  47.544678\n",
      " 27.194672  25.869316  19.249985  12.531376  19.984848  26.26664\n",
      " 24.017586  39.23136   35.420456  27.042572  44.375618  58.88925\n",
      " 52.534515  49.939545  39.07657   39.62393   38.10843   35.09149\n",
      " 33.057983  30.758514  38.18959   45.459457  26.815498 ]\n",
      "Predicted Standard Deviations:\n",
      "[ 4.4467554  8.191873   4.482628   8.406647   7.9414825  7.622196\n",
      "  8.473197   8.38373    4.582065   4.4745603  8.871214   3.971552\n",
      "  9.056245   8.752986   7.68107    4.2883     7.954897   8.658259\n",
      "  6.931597   8.340267  10.576478   6.779726   6.36308    6.202171\n",
      "  8.208863   4.420426   8.475369   7.455639   8.870515   8.852829\n",
      "  8.605621   8.7192135  6.4904714  4.4773293  7.9304695  8.349276\n",
      "  8.319981   4.1704283  8.826364   8.455806   8.652484   8.597094\n",
      "  8.599657   8.684054   8.213337   8.557248   8.103574   9.28817\n",
      "  4.4751945  8.029654   5.5289545  4.4541235  8.295761   8.400818\n",
      "  8.110215   9.004843   8.774448   9.023974   8.402254   8.416128\n",
      "  8.638184   8.918401   9.004843   8.361929  10.241074   7.7122264\n",
      "  8.617039   8.470699   7.297172   4.5150723  8.280544   4.812581\n",
      "  7.02017    8.237052   8.626348   8.640085   8.086576   9.060045\n",
      "  7.874387   9.258574   8.631811   8.410034   8.54316  ]\n"
     ]
    }
   ],
   "source": [
    "# Example usage in a GP model\n",
    "class MyGP(gpytorch.models.ExactGP): # i need to find a diferent model\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(MyGP, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.base_kernel = gpytorch.kernels.RBFKernel()\n",
    "        self.covar_module = DPkernel(base_kernel=self.base_kernel, num_dims=train_x.size(-1), q_additivity=train_x.size(-1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)  # Make sure to pass x twice WHY\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Create the GP model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "\n",
    "model = MyGP(train_x, y_train.squeeze(-1), likelihood)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    untrained_pred_dist = likelihood(model(test_x))\n",
    "    predictive_mean = untrained_pred_dist.mean\n",
    "    lower, upper = untrained_pred_dist.confidence_region()\n",
    "# Set up optimizer and marginal log likelihood\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "# Training loop\n",
    "training_iter = 1000\n",
    "for i in range(training_iter):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    # print(output)\n",
    "    loss = -mll(output, y_train)\n",
    "    loss = loss.mean() \n",
    "    loss.backward()\n",
    "    # print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "    #     i + 1, training_iter, loss.item(),\n",
    "    #     model.covar_module.base_kernel.lengthscale.item(),\n",
    "    #     model.likelihood.noise.item()\n",
    "    # ))\n",
    "    optimizer.step()\n",
    "# print('likelihood noise', likelihood.noise)\n",
    "# print('likelihood noise raw', likelihood.noise_covar.raw_noise)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    trained_pred_dist = likelihood(model(test_x))\n",
    "    predictive_mean = trained_pred_dist.mean\n",
    "    lower, upper = trained_pred_dist.confidence_region()\n",
    "# Viewing model parameters after training\n",
    "for param_name, param in model.named_parameters():\n",
    "    print(f'Parameter name: {param_name:42} value = {param.data}')\n",
    "\n",
    "\n",
    "#evaluating there is a problem when the test_y and test_x have float numbers\n",
    "with torch.no_grad():\n",
    "    model.eval()  # Set the model to evaluation mode (mode is for computing predictions through the model posterior.)\n",
    "    likelihood.eval()\n",
    "    output = likelihood(model(test_x))  # Make predictions on new data \n",
    "    \n",
    "\n",
    "\n",
    "for constraint_name, constraint in model.named_constraints():\n",
    "    print(f'Constraint name: {constraint_name:55} constraint = {constraint}')\n",
    "\n",
    "\n",
    "# Extracting means and standard deviations\n",
    "predicted_means = output.mean.numpy() \n",
    "predicted_stddevs = output.stddev.numpy()  # Extract standard deviations\n",
    "\n",
    "print(\"Predicted Means:\")\n",
    "print(predicted_means)\n",
    "\n",
    "print(\"Predicted Standard Deviations:\")\n",
    "print(predicted_stddevs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error 4.149475\n",
      "Mean Square Error 38.107735\n",
      "Root Mean Square Error 6.1731462\n"
     ]
    }
   ],
   "source": [
    "mae = mean_absolute_error(y_true=y_test, \n",
    "                          y_pred=predicted_means) \n",
    "print(\"Mean Absolute Error\", mae) \n",
    "\n",
    "mse = mean_squared_error(y_true=y_test, \n",
    "                          y_pred=predicted_means) \n",
    "print(\"Mean Square Error\", mse)\n",
    "\n",
    "\n",
    "rmse = mean_squared_error(y_true=y_test, \n",
    "                          y_pred=predicted_means,\n",
    "                          squared=False) \n",
    "print(\"Root Mean Square Error\", rmse) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the math formula DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: likelihood.noise_covar.raw_noise           value = tensor([10.7944])\n",
      "Parameter name: base_kernel.raw_lengthscale                value = tensor([[-0.2703]])\n",
      "Parameter name: covar_module.raw_outputscale               value = tensor([[18.9805,  0.4021, -4.8879, -4.8592, -3.6123,  3.6725]])\n",
      "Predicted Means: tensor([48.3056, 36.2659, 44.5181, 42.5665, 22.6022, 46.2165, 45.8330, 46.6031,\n",
      "        14.5454, 58.3750, 24.1471, 32.2372, 36.0581, 17.5024, 33.5036, 25.1483,\n",
      "        44.6882, 47.9840, 24.8952, 39.8115,  9.8097, 26.8850, 53.2394, 44.0092,\n",
      "        16.4627, 35.1913, 16.3916, 53.1917, 40.4400, 41.4790, 17.4635, 33.5219,\n",
      "        37.4490, 25.8741, 45.1173, 38.1199, 55.8328, 15.7393, 44.4786, 45.4272,\n",
      "        49.4690, 39.7816, 43.6416, 37.1267, 36.4754, 48.9983, 36.7415, 19.1290,\n",
      "        48.1863, 41.2861, 48.7412, 54.6846, 39.3551, 40.1428, 37.1681, 19.2500,\n",
      "        35.6286, 29.3131, 22.4671, 47.5451, 27.1948, 25.8696, 19.2500, 12.5314,\n",
      "        19.9850, 26.2668, 24.0178, 39.2321, 35.4210, 27.0428, 44.3763, 58.8899,\n",
      "        52.5352, 49.9401, 39.0772, 39.6244, 38.1087, 35.0920, 33.0585, 30.7586,\n",
      "        38.1899, 45.4599, 26.8157])\n",
      "Predicted Standard Deviations: tensor([ 2.9964,  7.5041,  3.0494,  7.7381,  7.2300,  6.8777,  7.8103,  7.7131,\n",
      "         3.1939,  3.0377,  8.2404,  2.2314,  8.4393,  8.1130,  6.9429,  2.7562,\n",
      "         7.2447,  8.0107,  6.1035,  7.6659, 10.0532,  5.9305,  5.4494,  5.2605,\n",
      "         7.5227,  2.9572,  7.8126,  6.6927,  8.2396,  8.2206,  7.9538,  8.0765,\n",
      "         5.5974,  3.0418,  7.2179,  7.6757,  7.6438,  2.5686,  8.1921,  7.7914,\n",
      "         8.0045,  7.9445,  7.9473,  8.0386,  7.5276,  7.9014,  7.4077,  8.6877,\n",
      "         3.0385,  7.3267,  4.4469,  3.0074,  7.6175,  7.7317,  7.4149,  8.3841,\n",
      "         8.1361,  8.4046,  7.7333,  7.7484,  7.9890,  8.2912,  8.3841,  7.6895,\n",
      "         9.6998,  6.9774,  7.9661,  7.8075,  6.5157,  3.0970,  7.6008,  3.5165,\n",
      "         6.2039,  7.5534,  7.9762,  7.9910,  7.3891,  8.4433,  7.1562,  8.6560,\n",
      "         7.9821,  7.7418,  7.8861])\n",
      "likelihood tensor([10.7945], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Viewing model parameters after training\n",
    "for param_name, param in model.named_parameters():\n",
    "    print(f'Parameter name: {param_name:42} value = {param.data}')\n",
    "\n",
    "# Ensure model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "likelihood.eval()\n",
    "with torch.no_grad():\n",
    "    #this is the training kernel matrix (K(X,X))\n",
    "    t_k_matrix = model.covar_module(train_x).evaluate()\n",
    "    #this is the noise matrix\n",
    "    n_matrix = torch.eye(t_k_matrix.size(-1)).to(t_k_matrix.device) * likelihood.noise_covar.raw_noise\n",
    "    # This is (K + sigma^2_n * I)^(-1)\n",
    "    K_inv = torch.inverse(t_k_matrix + n_matrix)\n",
    "    # alpha it the K_inv * y\n",
    "    alpha = torch.matmul(K_inv, model.train_targets.unsqueeze(-1))\n",
    "    # This is kernel matrix between the test points and training points, K(x*,X)\n",
    "    K_star = model.covar_module(test_x, train_x).evaluate()\n",
    "    #This is th ekernel matrix between all pairs of the test points (K(x*,x*))\n",
    "    K_star_star = model.covar_module(test_x).evaluate()\n",
    "\n",
    "    # Predictive mean calculation (mu_*)\n",
    "    pred_mean = torch.matmul(K_star, alpha)\n",
    "    # Predictive variance calculation (sigma^2_*)\n",
    "    pred_covar = K_star_star - torch.matmul(K_star, torch.matmul(K_inv, K_star.transpose(-1, -2)))\n",
    "\n",
    "    # Extract standard deviations from the covariance matrix\n",
    "    pred_stddev = torch.sqrt(torch.diag(pred_covar))\n",
    "\n",
    "# You can now print or return the predicted means and standard deviations\n",
    "print(\"Predicted Means:\", pred_mean.squeeze(-1))\n",
    "print(\"Predicted Standard Deviations:\", pred_stddev)\n",
    "print('likelihood', likelihood.noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error 4.149397\n",
      "Mean Square Error 38.10757\n",
      "Root Mean Square Error 6.173133\n"
     ]
    }
   ],
   "source": [
    "mae = mean_absolute_error(y_true=y_test, \n",
    "                          y_pred=pred_mean) \n",
    "print(\"Mean Absolute Error\", mae) \n",
    "\n",
    "mse = mean_squared_error(y_true=y_test, \n",
    "                          y_pred=pred_mean) \n",
    "print(\"Mean Square Error\", mse)\n",
    "\n",
    "\n",
    "rmse = mean_squared_error(y_true=y_test, \n",
    "                          y_pred=pred_mean,\n",
    "                          squared=False) \n",
    "print(\"Root Mean Square Error\", rmse) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained model MSE: 46.52, \n",
      "Trained model MSE: 38.11\n",
      "Untrained model MAE: 4.51, \n",
      "Trained model MAE: 4.15\n"
     ]
    }
   ],
   "source": [
    "\n",
    "init_mse = gpytorch.metrics.mean_squared_error(untrained_pred_dist, y_test, squared=True)\n",
    "final_mse = gpytorch.metrics.mean_squared_error(trained_pred_dist, y_test, squared=True)\n",
    "\n",
    "print(f'Untrained model MSE: {init_mse:.2f}, \\nTrained model MSE: {final_mse:.2f}')\n",
    "\n",
    "init_mae = gpytorch.metrics.mean_absolute_error(untrained_pred_dist, y_test)\n",
    "final_mae = gpytorch.metrics.mean_absolute_error(trained_pred_dist, y_test)\n",
    "\n",
    "print(f'Untrained model MAE: {init_mae:.2f}, \\nTrained model MAE: {final_mae:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
